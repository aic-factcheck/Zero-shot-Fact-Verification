#!/bin/bash
#SBATCH --time=24:00:00
#SBATCH --nodes=1 --ntasks-per-node=1 --cpus-per-task=4
#SBATCH --partition=amdgpu --gres=gpu:1
#SBATCH --mem=64G
#SBATCH --out=../logs/qa2d-cs-train_flan-t5.%j.out

# if PROJECT_DIR is not defined, then expect we are in ${PROJECT_DIR}/slurm
if [[ -z "${PROJECT_DIR}" ]]; then
    export PROJECT_DIR="$(dirname "$(pwd)")"
fi

if [ -f "${PROJECT_DIR}/init_environment_hflarge_amd.sh" ]; then
    source "${PROJECT_DIR}/init_environment_hflarge_amd.sh"
fi

cd ${PROJECT_DIR}

export PYTHONPATH=src:$PYTHONPATH

export DATASET=/mnt/data/factcheck/qa2d/cs

export LANG_SHORT=cs
export LANG=cs_CZ
# export SUFFIX="+unsupervised2M"

export DATASET=/mnt/data/factcheck/qa2d/$LANG_SHORT


export MODEL_SHORT="flan-t5-large"
export MODEL="ctu-aic/${MODEL_SHORT}"
export per_device_train_batch_size=16
export gradient_accumulation_steps=2

export EXP_DIR="qa2d/${MODEL}_${LANG}${SUFFIX}"
export WANDB_NAME="QA2D-${LANG_SHORT}: ${MODEL_SHORT}${SUFFIX}"
export WANDB_PROJECT="huggingface"

export EVAL_STEPS=128

export PYTHONPATH=.:$PYTHONPATH

    # --gradient_checkpointing \
    # --max_steps=3*1024 \
    ## --use_cache False \

python scripts/run_seq2seq.py \
    --model_name_or_path $MODEL \
    --do_train \
    --do_eval \
    --lang $LANG \
    --seed 42 \
    --train_file "$DATASET/train.jsonl" \
    --validation_file "$DATASET/dev.jsonl"  \
    --output_dir experiments/$EXP_DIR \
    --overwrite_output_dir \
    --use_fast_tokenizer \
    --per_device_train_batch_size $per_device_train_batch_size \
    --gradient_accumulation_steps $gradient_accumulation_steps \
    --bf16 \
    --input_columns "answer:question" \
    --target_columns turker_answer \
    --column_separator "</s>"\
    --optim "adamw_torch" \
    --learning_rate 2e-05 \
    --num_train_epochs 10 \
    --logging_first_step True \
    --logging_steps $EVAL_STEPS \
    --eval_steps $EVAL_STEPS \
    --save_steps $EVAL_STEPS \
    --evaluation_strategy "steps" \
    --save_strategy "steps" \
    --save_total_limit 3 \
    --load_best_model_at_end True \
    --prediction_loss_only True\
    --num_beams 10 \
    --max_source_length 1024 \
    --max_target_length 128