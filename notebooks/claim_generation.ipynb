{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "sys.path.append('Claim_Generation')\n",
    "\n",
    "from T5_QG import pipeline\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified version generating only SUPPORTED claims. Also fixed input and output formats for those we use in AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"/mnt/data/factcheck/claim_extraction/csfeversum/en/0.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(fname, object_pairs_hook=OrderedDict):\n",
    "    with open(fname, 'r') as json_file:\n",
    "        data = json.load(json_file, object_pairs_hook=object_pairs_hook)\n",
    "    return data\n",
    "\n",
    "def write_json(fname, data, indent=3):\n",
    "    with open(str(fname), 'w', encoding='utf8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=indent, default=str)\n",
    "\n",
    "\n",
    "def read_jsonl(jsonl):\n",
    "    with open(jsonl, 'r') as json_file:\n",
    "        data = []\n",
    "        for jline in json_file:\n",
    "            rec = json.loads(jline, object_pairs_hook=OrderedDict)\n",
    "            data.append(rec)\n",
    "    return data\n",
    "    \n",
    "\n",
    "def write_jsonl(jsonl, data):\n",
    "    # data is an iterable (list) of JSON-compatible structures (OrderedDict)\n",
    "    with open(jsonl, 'w', encoding='utf8') as json_file:\n",
    "        for r in data:\n",
    "            json.dump(r, json_file, ensure_ascii=False, default=str)\n",
    "            json_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 11:32:02 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-02-16 11:32:02 INFO: Use device: gpu\n",
      "2023-02-16 11:32:02 INFO: Loading: tokenize\n",
      "2023-02-16 11:32:08 INFO: Loading: ner\n",
      "2023-02-16 11:32:10 INFO: Done loading processors!\n",
      "100%|██████████| 42383/42383 [49:25<00:00, 14.29it/s]  \n"
     ]
    }
   ],
   "source": [
    "def extract_ners(input_jsonl, ner_json):\n",
    "    # for each text gives a triplet (ner, ner_type, ner-ner_type count in text)\n",
    "    # the triplets are sorted by decreasing count\n",
    "    stanza_nlp = stanza.Pipeline('en', use_gpu = True, processors=\"tokenize,ner\")\n",
    "    entity_dict = OrderedDict()\n",
    "    src = read_jsonl(input_jsonl)\n",
    "    for l in tqdm(src):\n",
    "        text = l[\"text\"]\n",
    "        pass_doc = stanza_nlp(text)\n",
    "        ner_pairs = [(ent.text, ent.type) for ent in pass_doc.ents] # text-type pairs\n",
    "        ner_cnts = Counter(ner_pairs) # their \n",
    "        ners_unique_with_counts =  [(p[0], p[1], ner_cnts[(p[0], p[1])]) for p in set(ner_pairs)]\n",
    "        ners_unique_with_counts = sorted(ners_unique_with_counts, key=lambda n: -n[2])\n",
    "        entity_dict[l[\"id\"]] = ners_unique_with_counts\n",
    "    write_json(ner_json, entity_dict)\n",
    "\n",
    "# extract_ners(Path(DATA_DIR, \"test.jsonl\"), Path(DATA_DIR, \"qacg\", \"test_ners.json\"))\n",
    "extract_ners(Path(DATA_DIR, \"train.jsonl\"), Path(DATA_DIR, \"qacg\", \"train_ners.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QG module >>>>>>>>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d77936bb0fb469aad2f7f5597eabf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22c7f912ed14bbe94b371bf51ee4006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e799265a4b46d5abc4c565756f8974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0ab573fb144b98a2b89d24ad29dd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/15.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566463f6e78042c38c50358f231f22d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecbce661e854fa69a56ed6a2ae844d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71a88005b4b4695931d7a194fbda4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91def5ac180e430097bab4c5ee16c5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab720c8119c42748c31ac5305618228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47000e0569b04bc88b363eb8830844e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5536ff1d9f49f18a8dba62705476c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6005b79110a3438880326ecb40bc5ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QG module loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3479/42383 [1:29:47<15:16:24,  1.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3482/42383 [1:29:51<13:56:17,  1.29s/it]"
     ]
    }
   ],
   "source": [
    "def generate_qas(input_jsonl, ner_json, qas_json):\n",
    "    # QG NLP object\n",
    "    gpu_index = 0\n",
    "\n",
    "    print('Loading QG module >>>>>>>>')\n",
    "    qg_nlp = pipeline(\"question-generation\", model='valhalla/t5-base-qg-hl', qg_format=\"highlight\", gpu_index = gpu_index)\n",
    "    print('QG module loaded.')\n",
    "\n",
    "    src = read_jsonl(input_jsonl)\n",
    "    ners = read_json(ner_json)\n",
    "\n",
    "    qas = OrderedDict()\n",
    "    invalid_sample = 0\n",
    "    for l in tqdm(src):\n",
    "        entities = ners[str(l['id'])]\n",
    "\n",
    "        # create a batch\n",
    "        sources, answers = [], []\n",
    "        for ent_text, ent_type, ent_cnt in entities:\n",
    "            sources.append(l['text'])\n",
    "            answers.append(ent_text)\n",
    "            \n",
    "        # question generation\n",
    "        if len(sources) > 0 and len(sources) == len(answers):\n",
    "            results = []\n",
    "            try:\n",
    "                results = qg_nlp.batch_qg_with_answer(sources, answers)\n",
    "            except:\n",
    "                invalid_sample += 1\n",
    "\n",
    "            if len(results) == 0:\n",
    "                continue\n",
    "            \n",
    "            # save results\n",
    "            result_for_sample = {}\n",
    "            for ind, QA in enumerate(results):\n",
    "                ent_text, ent_type, _ = entities[ind]\n",
    "                question = QA['question']\n",
    "                answer = QA['answer']\n",
    "                result_for_sample[f'{ent_text}:::{ent_type}'] = [question, answer]\n",
    "\n",
    "            qas[str(l['id'])] = result_for_sample\n",
    "        else:\n",
    "            invalid_sample += 1\n",
    "\n",
    "    print(f'#invalid samples: {invalid_sample}')\n",
    "    write_json(qas_json, qas)\n",
    "\n",
    "\n",
    "# generate_qas(Path(DATA_DIR, \"test.jsonl\"), Path(DATA_DIR, \"qacg\", \"test_ners.json\"), Path(DATA_DIR, \"qacg\", \"test_qas.json\"))\n",
    "generate_qas(Path(DATA_DIR, \"train.jsonl\"), Path(DATA_DIR, \"qacg\", \"train_ners.json\"), Path(DATA_DIR, \"qacg\", \"train_qas.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_claims(input_jsonl, ner_json, qas_json, claims_json, QA2D_model_path, gpu_index=0):\n",
    "    # QA2D model object\n",
    "    print('Loading QA2D module >>>>>>>>')\n",
    "    model_args = Seq2SeqArgs()\n",
    "    model_args.max_length = 64\n",
    "    model_args.silent = True\n",
    "\n",
    "    QA2D_model = Seq2SeqModel(\n",
    "        encoder_decoder_type=\"bart\", \n",
    "        encoder_decoder_name=QA2D_model_path,\n",
    "        cuda_device=gpu_index,\n",
    "        args=model_args\n",
    "    )\n",
    "\n",
    "    src = read_jsonl(input_jsonl)\n",
    "    ners = read_json(ner_json)\n",
    "    qas = read_json(qas_json)\n",
    "\n",
    "    def claims_for_sample(sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = []\n",
    "        for ent_text, ent_type, _ in ners[id_]:\n",
    "            passage_entities.append(f'{ent_text}:::{ent_type}')\n",
    "        if len(passage_entities) == 0:\n",
    "            # no NERs\n",
    "            return None \n",
    "\n",
    "        # Step 2: load precomputed QAs for entities\n",
    "        if id_ not in qas:\n",
    "            print(f\"missing id: {id_}\")\n",
    "            return None\n",
    "        QA_for_sample = qas[id_]\n",
    "        QA_pairs = []\n",
    "        for entity in passage_entities:\n",
    "            if entity in QA_for_sample:\n",
    "                question, answer = QA_for_sample[entity]\n",
    "                QA_pairs.append({'question': question, 'answer': answer})\n",
    "            else:\n",
    "                print(f\"missing entity: {entity} for id: {id_}\")\n",
    "                return None\n",
    "        if len(QA_pairs) == 0:\n",
    "            print(f\"zero length pairs for id: {id_}\")\n",
    "            return None\n",
    "\n",
    "        # Step 3: QA2D\n",
    "        to_predict = [qa['question'] + ' [SEP] ' + qa['answer'] for qa in QA_pairs]\n",
    "        results = []\n",
    "        # try:\n",
    "        results = QA2D_model.predict(to_predict)\n",
    "        # except:\n",
    "            # return None\n",
    "        if len(results) == 0:\n",
    "            print(f\"zero length results for id: {id_}\")\n",
    "            return None\n",
    "\n",
    "        assert len(results) == len(QA_pairs)\n",
    "\n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(passage_entities, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "    generated_claims = OrderedDict()\n",
    "    for sample in tqdm(src[:]):\n",
    "        id_ = str(sample['id'])\n",
    "        claims = claims_for_sample(sample)\n",
    "        if claims is None:\n",
    "            claims = {}\n",
    "        generated_claims[id_] = claims\n",
    "\n",
    "    write_json(claims_json, generated_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 4058/5288 [1:12:18<13:58,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing id: 33582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 4209/5288 [1:15:10<29:11,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing id: 11387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 4328/5288 [1:17:25<14:17,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing id: 10898\n",
      "missing id: 10956\n",
      "missing id: 12681\n",
      "missing id: 12937\n",
      "missing id: 14703\n",
      "missing id: 16676\n",
      "missing id: 18741\n",
      "missing id: 18985\n",
      "missing id: 19645\n",
      "missing id: 23393\n",
      "missing id: 23443\n",
      "missing id: 26663\n",
      "missing id: 28920\n",
      "missing id: 29516\n",
      "missing id: 30371\n",
      "missing id: 30613\n",
      "missing id: 31112\n",
      "missing id: 31325\n",
      "missing id: 34091\n",
      "missing id: 35831\n",
      "missing id: 38082\n",
      "missing id: 40064\n",
      "missing id: 40149\n",
      "missing id: 42307\n",
      "missing id: 45006\n",
      "missing id: 45622\n",
      "missing id: 46943\n",
      "missing id: 49045\n",
      "missing id: 51562\n",
      "missing id: 54051\n",
      "missing id: 59046\n",
      "missing id: 65215\n",
      "missing id: 6899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 4633/5288 [1:21:53<09:13,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing id: 11537\n",
      "missing id: 12943\n",
      "missing id: 13271\n",
      "missing id: 13442\n",
      "missing id: 13651\n",
      "missing id: 13980\n",
      "missing id: 14049\n",
      "missing id: 15454\n",
      "missing id: 15749\n",
      "missing id: 17596\n",
      "missing id: 18178\n",
      "missing id: 18589\n",
      "missing id: 20040\n",
      "missing id: 21972\n",
      "missing id: 22560\n",
      "missing id: 25971\n",
      "missing id: 26397\n",
      "missing id: 27224\n",
      "missing id: 28231\n",
      "missing id: 28999\n",
      "missing id: 29643\n",
      "missing id: 30025\n",
      "missing id: 30544\n",
      "missing id: 30693\n",
      "missing id: 31944\n",
      "missing id: 32674\n",
      "missing id: 35182\n",
      "missing id: 35637\n",
      "missing id: 38876\n",
      "missing id: 40427\n",
      "missing id: 41853\n",
      "missing id: 43778\n",
      "missing id: 44256\n",
      "missing id: 44399\n",
      "missing id: 47566\n",
      "missing id: 48167\n",
      "missing id: 48216\n",
      "missing id: 49577\n",
      "missing id: 50056\n",
      "missing id: 51427\n",
      "missing id: 51450\n",
      "missing id: 52280\n",
      "missing id: 54286\n",
      "missing id: 54292\n",
      "missing id: 54921\n",
      "missing id: 57709\n",
      "missing id: 58912\n",
      "missing id: 61076\n",
      "missing id: 61761\n",
      "missing id: 62458\n",
      "missing id: 64294\n",
      "missing id: 64875\n",
      "missing id: 65908\n",
      "missing id: 66854\n",
      "missing id: 67035\n",
      "missing id: 67879\n",
      "missing id: 67923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5288/5288 [1:30:31<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate_claims(\n",
    "#     input_jsonl=Path(DATA_DIR, \"test.jsonl\"),\n",
    "#     ner_json=Path(DATA_DIR, \"qacg\", \"test_ners.json\"),\n",
    "#     qas_json=Path(DATA_DIR, \"qacg\", \"test_qas.json\"),\n",
    "#     claims_json=Path(DATA_DIR, \"qacg\", \"test_claims.json\"),\n",
    "#     QA2D_model_path=\"dependencies/QA2D_model\",\n",
    "#     gpu_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_claims(\n",
    "    input_jsonl=Path(DATA_DIR, \"train.jsonl\"),\n",
    "    ner_json=Path(DATA_DIR, \"qacg\", \"train_ners.json\"),\n",
    "    qas_json=Path(DATA_DIR, \"qacg\", \"train_qas.json\"),\n",
    "    claims_json=Path(DATA_DIR, \"qacg\", \"train_claims.json\"),\n",
    "    QA2D_model_path=\"dependencies/QA2D_model\",\n",
    "    gpu_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env_plight_env",
   "language": "python",
   "name": "fc_env_plight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
