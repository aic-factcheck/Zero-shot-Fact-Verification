{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "from itertools import chain\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import unicodedata as ud\n",
    "from time import time\n",
    "from typing import Dict, Type, Callable, List, Union\n",
    "import sys\n",
    "import ujson\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.fever import fever_detokenize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** move elsewhere NLI models should be covered in own package. Currently it is here for convenience only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH = \"full\" # all generated data\n",
    "# APPROACH = \"balanced\" # balanced classes\n",
    "APPROACH = \"balance_shuf\" # balanced classes, shuffled\n",
    "# APPROACH = \"fever_size\" # QACG data subsampled to Cs/EnFEVER dataset size\n",
    "\n",
    "LANG, NER_DIR = \"cs\", \"PAV-ner-CNEC\"\n",
    "# LANG, NER_DIR = \"en\", \"stanza\"\n",
    "# LANG, NER_DIR = \"sk\", \"crabz_slovakbert-ner\" \n",
    "# LANG, NER_DIR = \"pl\", \"stanza\"\n",
    "DATE = \"20230801\"\n",
    "\n",
    "QG_DIR = \"mt5-large_all-cp126k\"\n",
    "QACG_DIR = \"mt5-large_all-cp156k\"\n",
    "\n",
    "# BELOW configuration is language-agnostic\n",
    "\n",
    "WIKI_ROOT = f\"/mnt/data/factcheck/wiki/{LANG}/{DATE}\"\n",
    "WIKI_CORPUS = f\"{WIKI_ROOT}/paragraphs/{LANG}wiki-{DATE}-paragraphs.jsonl\"\n",
    "\n",
    "QACG_ROOT = f\"{WIKI_ROOT}/qacg\"\n",
    "\n",
    "NLI_DIR = Path(\"nli\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "NLI_ROOT = Path(QACG_ROOT, NLI_DIR)\n",
    "\n",
    "SPLIT_DIR = Path(\"splits\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "SPLIT_ROOT = Path(QACG_ROOT, SPLIT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008602380752563477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a47cbd02bfe4e5a94c0c10e976bd9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def import_corpus(corpus_file):\n",
    "    # it already has correct format\n",
    "    raw = read_jsonl(corpus_file, show_progress=True)\n",
    "    for e in raw:\n",
    "        e[\"id\"] = nfc(e[\"id\"])\n",
    "        e[\"did\"] = nfc(e[\"did\"])\n",
    "        e[\"text\"] = nfc(e[\"text\"])\n",
    "    return raw\n",
    "\n",
    "\n",
    "def generate_original_id2pid_mapping(corpus):\n",
    "    original_id2pid = {}\n",
    "    for pid, r in enumerate(corpus):\n",
    "        original_id = r[\"id\"]\n",
    "        # assert original_id not in original_id2pid, f\"original ID not unique! {original_id}\"\n",
    "        if original_id in original_id2pid:\n",
    "            print(f\"original ID not unique! {pid} {original_id}, previous pid: {original_id2pid[original_id]}\")\n",
    "        original_id2pid[original_id] = pid\n",
    "    return original_id2pid\n",
    "\n",
    "corpus = import_corpus(WIKI_CORPUS)\n",
    "original_id2pid = generate_original_id2pid_mapping(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107330/107330 [00:00<00:00, 733903.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 107330, label counts: Counter({'s': 53542, 'n': 35639, 'r': 18149}) to:\n",
      " /mnt/data/factcheck/wiki/cs/20230801/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/train_fever_size.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 908632.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 9999, label counts: Counter({'n': 3333, 'r': 3333, 's': 3333}) to:\n",
      " /mnt/data/factcheck/wiki/cs/20230801/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_fever_size.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 963402.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 9999, label counts: Counter({'s': 3333, 'r': 3333, 'n': 3333}) to:\n",
      " /mnt/data/factcheck/wiki/cs/20230801/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/test_fever_size.jsonl\n"
     ]
    }
   ],
   "source": [
    "def prepare_nli_data(src_file, dst_file, corpus, original_id2pid, seed=1234):\n",
    "    # imports data created for Evidence retrieval (ColBERTv2:prepare_data_wiki.ipynb)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    recs = []\n",
    "    counts = Counter()\n",
    "    data = read_jsonl(src_file)\n",
    "    for sample in tqdm(data):\n",
    "        claim = sample[\"claim\"]\n",
    "        label = sample[\"label\"]\n",
    "        evidence_bids = sample[\"evidence\"]\n",
    "        assert len(evidence_bids) == 1, \"More than single evidence not impemented (yet)\" \n",
    "        context = corpus[original_id2pid[evidence_bids[0]]][\"text\"]\n",
    "        recs.append({\"claim\": claim, \"context\": context, \"label\": label})\n",
    "        counts[label] += 1\n",
    "    rng.shuffle(recs)\n",
    "    print(f\"exporting {len(recs)}, label counts: {counts} to:\\n {str(dst_file)}\")\n",
    "    write_jsonl(dst_file, recs, mkdir=True)\n",
    "\n",
    "prepare_nli_data(Path(SPLIT_ROOT, f\"train_{APPROACH}.jsonl\"), Path(NLI_ROOT, f\"train_{APPROACH}.jsonl\"), corpus, original_id2pid, seed=1234)\n",
    "prepare_nli_data(Path(SPLIT_ROOT, f\"dev_{APPROACH}.jsonl\"), Path(NLI_ROOT, f\"dev_{APPROACH}.jsonl\"), corpus, original_id2pid, seed=1235)\n",
    "prepare_nli_data(Path(SPLIT_ROOT, f\"test_{APPROACH}.jsonl\"), Path(NLI_ROOT, f\"test_{APPROACH}.jsonl\"), corpus, original_id2pid, seed=1236)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning differing lengths: [186489, 183204, 188364, 183485]\n",
      "sampling 4 x 46623 = 186492, 3 over to 186489 to: /mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/qacg/splits/train_balanced_shuf.jsonl\n",
      "Warning differing lengths: [18750, 18964, 18685, 19174]\n",
      "sampling 4 x 4688 = 18752, 2 over to 18750 to: /mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/qacg/splits/dev_balanced_shuf.jsonl\n",
      "Warning differing lengths: [18146, 18003, 17731, 17727]\n",
      "sampling 4 x 4537 = 18148, 2 over to 18146 to: /mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/qacg/splits/test_balanced_shuf.jsonl\n"
     ]
    }
   ],
   "source": [
    "def create_combined_split(src_files, dst_files, rng):\n",
    "    # combined dataset has the same size as each individual language dataset\n",
    "    data = [read_jsonl(src_file[1]) for src_file in src_files]\n",
    "    langs = [src_file[0] for src_file in src_files]\n",
    "    n = len(data)\n",
    "    lens = list(set([len(d) for d in data]))\n",
    "    if not all(l == lens[0] for l in lens[1:]):\n",
    "        print(f\"Warning differing lengths: {lens}\")\n",
    "    l = lens[0]\n",
    "    psize = math.ceil(l/n)\n",
    "    over = n*psize - l\n",
    "    print(f\"sampling {n} x {psize} = {n*psize}, {over} over to {l} to: {dst_files}\")\n",
    "    recs = []\n",
    "    for lang, d in zip(langs, data):\n",
    "        indices = rng.choice(len(d), psize, replace=False)\n",
    "        rec = []\n",
    "        for idx in indices:\n",
    "            r = d[idx]\n",
    "            r[\"lang\"] = lang\n",
    "            r[\"orig_idx\"] = idx # the index in the original language claim file\n",
    "            rec.append(r)\n",
    "        recs += list(rec)\n",
    "    del recs[:over]\n",
    "    rng.shuffle(recs)\n",
    "    assert len(recs) == l\n",
    "    write_jsonl(dst_files, recs, mkdir=True)\n",
    "\n",
    "\n",
    "APPROACH = \"balanced_shuf\"\n",
    "rng = np.random.RandomState(1234)\n",
    "for split in [f\"train_{APPROACH}.jsonl\", f\"dev_{APPROACH}.jsonl\", f\"test_{APPROACH}.jsonl\"]:\n",
    "    create_combined_split([\n",
    "        (\"cs\", Path(\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"en\", Path(\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"pl\", Path(\"/mnt/data/factcheck/wiki/pl/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"sk\", Path(\"/mnt/data/factcheck/wiki/sk/20230801/qacg/splits/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        ],\n",
    "        Path(\"/mnt/data/factcheck/wiki/cs_en_pl_sk/20230801/qacg/splits\", split),\n",
    "        rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sum_split(src_files, dst_files, rng):\n",
    "    # the sum dataset simply concatenates (and shuffles) all source language datasets\n",
    "    data = [read_jsonl(src_file[1]) for src_file in src_files]\n",
    "    langs = [src_file[0] for src_file in src_files]\n",
    "    recs = []\n",
    "    for lang, d in zip(langs, data):\n",
    "        indices = range(len(d))\n",
    "        rec = []\n",
    "        for idx in indices:\n",
    "            r = d[idx]\n",
    "            r[\"lang\"] = lang\n",
    "            r[\"orig_idx\"] = idx # the index in the original language claim file\n",
    "            rec.append(r)\n",
    "        recs += list(rec)\n",
    "    rng.shuffle(recs)\n",
    "    write_jsonl(dst_files, recs, mkdir=True)\n",
    "\n",
    "\n",
    "APPROACH = \"balanced_shuf\"\n",
    "rng = np.random.RandomState(1234)\n",
    "for split in [f\"train_{APPROACH}.jsonl\", f\"dev_{APPROACH}.jsonl\", f\"test_{APPROACH}.jsonl\"]:\n",
    "    create_sum_split([\n",
    "        (\"cs\", Path(\"/mnt/data/factcheck/wiki/cs/20230801/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"en\", Path(\"/mnt/data/factcheck/wiki/en/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"pl\", Path(\"/mnt/data/factcheck/wiki/pl/20230801/qacg/splits/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"sk\", Path(\"/mnt/data/factcheck/wiki/sk/20230801/qacg/splits/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        ],\n",
    "        Path(\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/qacg/splits\", split),\n",
    "        rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nli_data_combined(src_files, dst_files, lang2fcorpus, seed=1234):\n",
    "    # imports data created for Evidence retrieval (ColBERTv2:prepare_data_wiki.ipynb)\n",
    "    srcs = [read_jsonl(src_file) for src_file in tqdm(src_files, desc=\"reading sources\")]\n",
    "\n",
    "    for lang, fcorpus in lang2fcorpus.items():\n",
    "        print(f\"loading corpus for {lang.upper()} from '{fcorpus}'\")\n",
    "        corpus = import_corpus(fcorpus)\n",
    "        original_id2pid = generate_original_id2pid_mapping(corpus)\n",
    "        for src in srcs:\n",
    "            for sample in src:\n",
    "                if sample[\"lang\"] == lang:\n",
    "                    evidence_bids = sample[\"evidence\"]\n",
    "                    assert len(evidence_bids) == 1, \"More than single evidence not impemented (yet)\" \n",
    "                    context = corpus[original_id2pid[evidence_bids[0]]][\"text\"]\n",
    "                    sample[\"context\"] = context\n",
    "    for src, dst_file in zip(srcs, dst_files):\n",
    "        print(f\"exporting {len(src)} to:\\n {str(dst_file)}\")\n",
    "        write_jsonl(dst_file, src, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '20230801'\n",
    "prepare_nli_data_combined(\n",
    "    src_files=[\n",
    "        f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/qacg/splits/dev_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/qacg/splits/test_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/qacg/splits/train_balanced.jsonl\",\n",
    "    ],\n",
    "    dst_files=[\n",
    "        f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/qacg/nli/dev_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/qacg/nli/test_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/cs_en_pl_sk/{DATE}/qacg/nli/train_balanced.jsonl\",\n",
    "    ],\n",
    "    lang2fcorpus={\n",
    "        \"cs\": f\"/mnt/data/factcheck/wiki/cs/{DATE}/paragraphs/cswiki-{DATE}-paragraphs.jsonl\",\n",
    "        \"en\": f\"/mnt/data/factcheck/wiki/en/{DATE}/paragraphs/enwiki-{DATE}-paragraphs.jsonl\",\n",
    "        \"pl\": f\"/mnt/data/factcheck/wiki/pl/{DATE}/paragraphs/plwiki-{DATE}-paragraphs.jsonl\",\n",
    "        \"sk\": f\"/mnt/data/factcheck/wiki/sk/{DATE}/paragraphs/skwiki-{DATE}-paragraphs.jsonl\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading sources: 100%|██████████| 3/3 [00:07<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for CS from '/mnt/data/factcheck/wiki/cs/20230801/paragraphs/cswiki-20230801-paragraphs.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009677886962890625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6166b40b4bde497e956f752e95454484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for EN from '/mnt/data/factcheck/wiki/en/20230801/paragraphs/enwiki-20230801-paragraphs.jsonl'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009851694107055664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047a3076dfe145aebb58434d11548eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for PL from '/mnt/data/factcheck/wiki/pl/20230801/paragraphs/plwiki-20230801-paragraphs.jsonl'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009086370468139648,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed24e1c64c3452893f3f190bcd2c7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for SK from '/mnt/data/factcheck/wiki/sk/20230801/paragraphs/skwiki-20230801-paragraphs.jsonl'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.300494909286499,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9485083598461a9462f5855ef1eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 120348 to:\n",
      " /mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/qacg/nli/dev_balanced.jsonl\n",
      "exporting 113760 to:\n",
      " /mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/qacg/nli/test_balanced.jsonl\n",
      "exporting 1180836 to:\n",
      " /mnt/data/factcheck/wiki/sum_cs_en_pl_sk/20230801/qacg/nli/train_balanced.jsonl\n"
     ]
    }
   ],
   "source": [
    "# use the same procedure for the SUM dataset\n",
    "DATE = '20230801'\n",
    "prepare_nli_data_combined(\n",
    "    src_files=[\n",
    "        f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/qacg/splits/dev_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/qacg/splits/test_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/qacg/splits/train_balanced.jsonl\",\n",
    "    ],\n",
    "    dst_files=[\n",
    "        f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/qacg/nli/dev_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/qacg/nli/test_balanced.jsonl\",\n",
    "        f\"/mnt/data/factcheck/wiki/sum_cs_en_pl_sk/{DATE}/qacg/nli/train_balanced.jsonl\",\n",
    "    ],\n",
    "    lang2fcorpus={\n",
    "        \"cs\": f\"/mnt/data/factcheck/wiki/cs/{DATE}/paragraphs/cswiki-{DATE}-paragraphs.jsonl\",\n",
    "        \"en\": f\"/mnt/data/factcheck/wiki/en/{DATE}/paragraphs/enwiki-{DATE}-paragraphs.jsonl\",\n",
    "        \"pl\": f\"/mnt/data/factcheck/wiki/pl/{DATE}/paragraphs/plwiki-{DATE}-paragraphs.jsonl\",\n",
    "        \"sk\": f\"/mnt/data/factcheck/wiki/sk/{DATE}/paragraphs/skwiki-{DATE}-paragraphs.jsonl\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/wiki/sk/20230801/qacg/nli/crabz_slovakbert-ner/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLI_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/drchajan/.cache/huggingface/datasets/json/default-696677cf81c28f6c/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010984659194946289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e6cb192f544b1f86a32cc42d0590ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_nli = load_dataset(\"json\", data_files={\n",
    "    \"train\": str(Path(NLI_ROOT, \"train_balanced.jsonl\")),\n",
    "    \"dev\": str(Path(NLI_ROOT, \"dev_balanced.jsonl\")),\n",
    "    \"test\": str(Path(NLI_ROOT, \"test_balanced.jsonl\"))\n",
    "    })\n",
    "\n",
    "# raw_nli = load_dataset(\"json\", data_files={\n",
    "#     \"train\": str(Path(CLAIM_ROOT, \"train_nli_sr.jsonl\")),\n",
    "#     \"dev\": str(Path(CLAIM_ROOT, \"dev_nli_sr.jsonl\")),\n",
    "#     \"test\": str(Path(CLAIM_ROOT, \"test_nli_sr.jsonl\"))\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "dev\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for a in raw_nli.:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(raw_nli[\"train\"][\"context\"]):\n",
    "    if len(c) < 30:\n",
    "        print(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/fc_env_hflarge/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/fc_env_hflarge/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/google/flan-t5-base_cs_CZ/checkpoint-896\"\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/google/flan-t5-large_cs_CZ/checkpoint-1568\"\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/google/flan-t5-large_cs_CZ-20230801/checkpoint-6144\"\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/google/flan-t5-large_cs_CZ-20230801/checkpoint-23936\"\n",
    "\n",
    "# SUPPORT/REFUTE only models\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/google/umt5-base_cs_CZ-20230801_sr/checkpoint-23936\"\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/google/flan-t5-large_cs_CZ-20230801_sr/checkpoint-256\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "# Encoder (Softmax) models\n",
    "# model_id = \"ctu-aic/xlm-roberta-large-xnli-csfever\"\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/deepset/xlm-roberta-large-squad2_cs_CZ-20230801_lr1e-6/checkpoint-41792\"\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/deepset/xlm-roberta-large-squad2_en_US-20230220_lr1e-6/checkpoint-48416\"\n",
    "# model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli_fever/deepset/xlm-roberta-large-squad2_en_US_lr1e-6/checkpoint-132864\"\n",
    "model_id = \"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/nli/deepset/xlm-roberta-large-squad2_sk_SK-20230801_balanced_lr1e-6/checkpoint-202112\"\n",
    "\n",
    "id2label = {0: \"s\", 1: \"r\", 2: \"n\"}\n",
    "label2id = {\"s\": 0, \"r\": 1, \"n\": 1}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_id, device_map=\"auto\", id2label=id2label, label2id=label2id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008521795272827148,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 223,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd696a4adb4943b9aefc001e7aa557b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Y, C, T\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Y, C, T = split_predict(model, raw_nli[\"test\"], device=\"cuda\", max_length=128) # FAST\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Y, C, T = split_predict(model, raw_nli[\"test\"], device=\"cuda\", max_length=512) # CORRECT, set to model maximum input length\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m Y, C, T \u001b[39m=\u001b[39m split_predict(model, raw_nli[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m], device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m) \u001b[39m# CORRECT, set to model maximum input length\u001b[39;00m\n",
      "\u001b[1;32m/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m inputs \u001b[39m=\u001b[39m [[claim, context] \u001b[39mfor\u001b[39;00m claim, context \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(split[\u001b[39m\"\u001b[39m\u001b[39mclaim\u001b[39m\u001b[39m\"\u001b[39m],  split[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m])]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# inputs = [[context, claim] for claim, context in zip(split[\"claim\"],  split[\"context\"])] # SWITCHED CTX and CLAIM!!!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m Ys \u001b[39m=\u001b[39m batch_apply(predict, inputs, batch_size\u001b[39m=\u001b[39;49mbatch_size, show_progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m Y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvstack(Ys)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m C \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mid2label[id_\u001b[39m.\u001b[39mitem()] \u001b[39mfor\u001b[39;00m id_ \u001b[39min\u001b[39;00m Y\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m~/devel/python/FC/aic-nlp-utils/aic_nlp_utils/batch.py:22\u001b[0m, in \u001b[0;36mbatch_apply\u001b[0;34m(func, data, batch_size, show_progress)\u001b[0m\n\u001b[1;32m     20\u001b[0m iter_ \u001b[39m=\u001b[39m tqdm(ranges_, total\u001b[39m=\u001b[39mn_batches) \u001b[39mif\u001b[39;00m show_progress \u001b[39melse\u001b[39;00m ranges_\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m f, l \u001b[39min\u001b[39;00m iter_:\n\u001b[0;32m---> 22\u001b[0m     r \u001b[39m=\u001b[39m func(data[f:l])\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(r, Iterable):\n\u001b[1;32m     24\u001b[0m         r \u001b[39m=\u001b[39m [r]\n",
      "\u001b[1;32m/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m X[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     Y \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brci/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/notebooks/nli_qacg.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Y\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/devel/python/FC/THIRDPARTY/transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1206\u001b[0m, in \u001b[0;36mXLMRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1206\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1207\u001b[0m     input_ids,\n\u001b[1;32m   1208\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1209\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1210\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1211\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1212\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1213\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1214\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1215\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1216\u001b[0m )\n\u001b[1;32m   1217\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1218\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/devel/python/FC/THIRDPARTY/transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:846\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    837\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    839\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    840\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    841\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    844\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    845\u001b[0m )\n\u001b[0;32m--> 846\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    847\u001b[0m     embedding_output,\n\u001b[1;32m    848\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    849\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    850\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    851\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    852\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    853\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    854\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    855\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    856\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    857\u001b[0m )\n\u001b[1;32m    858\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    859\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/devel/python/FC/THIRDPARTY/transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:530\u001b[0m, in \u001b[0;36mXLMRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    521\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    522\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    523\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    531\u001b[0m         hidden_states,\n\u001b[1;32m    532\u001b[0m         attention_mask,\n\u001b[1;32m    533\u001b[0m         layer_head_mask,\n\u001b[1;32m    534\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    535\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    536\u001b[0m         past_key_value,\n\u001b[1;32m    537\u001b[0m         output_attentions,\n\u001b[1;32m    538\u001b[0m     )\n\u001b[1;32m    540\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/devel/python/FC/THIRDPARTY/transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:456\u001b[0m, in \u001b[0;36mXLMRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    453\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    454\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 456\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    457\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    458\u001b[0m )\n\u001b[1;32m    459\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    461\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/devel/python/FC/THIRDPARTY/transformers/src/transformers/pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/devel/python/FC/THIRDPARTY/transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:469\u001b[0m, in \u001b[0;36mXLMRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    468\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 469\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/devel/python/FC/THIRDPARTY/transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py:380\u001b[0m, in \u001b[0;36mXLMRobertaOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 380\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    381\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    382\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.0.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from aic_nlp_utils.batch import batch_apply\n",
    "\n",
    "def split_predict(model, split, batch_size=128, device=\"cuda\", max_length=128):\n",
    "    def predict(inputs):\n",
    "        X = tokenizer(inputs, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = X[\"input_ids\"].to(device)\n",
    "        attention_mask = X[\"attention_mask\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            Y = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            return Y\n",
    "        \n",
    "    inputs = [[claim, context] for claim, context in zip(split[\"claim\"],  split[\"context\"])]\n",
    "    # inputs = [[context, claim] for claim, context in zip(split[\"claim\"],  split[\"context\"])] # SWITCHED CTX and CLAIM!!!\n",
    "    Ys = batch_apply(predict, inputs, batch_size=batch_size, show_progress=True)\n",
    "    Y = torch.vstack(Ys)\n",
    "    C = [model.config.id2label[id_.item()] for id_ in Y.argmax(dim=1)]\n",
    "    T = [l for l in split[\"label\"]]\n",
    "    return Y, C, T\n",
    "\n",
    "# Y, C, T = split_predict(model, raw_nli[\"test\"], device=\"cuda\", max_length=128) # FAST\n",
    "# Y, C, T = split_predict(model, raw_nli[\"test\"], device=\"cuda\", max_length=512) # CORRECT, set to model maximum input length\n",
    "Y, C, T = split_predict(model, raw_nli[\"test\"], device=\"cpu\", max_length=512) # CORRECT, set to model maximum input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6492715709710883\n",
      "F1: 0.5975708877212096\n",
      "\n",
      "cm:\n",
      "[[7657  233  210]\n",
      " [3508 1488 1310]\n",
      " [ 591 1924 5250]]\n",
      "\n",
      "C=Counter({'s': 11756, 'n': 6770, 'r': 3645})\n",
      "T=Counter({'s': 8100, 'n': 7765, 'r': 6306})\n"
     ]
    }
   ],
   "source": [
    "print(f\"acc: {accuracy_score(T, C)}\")\n",
    "print(f\"F1: {f1_score(T, C, average='macro')}\")\n",
    "# print(f\"cm:\\n{confusion_matrix(T, C)}\")\n",
    "print()\n",
    "print(f\"cm:\\n{confusion_matrix(T, C, labels=['s', 'r', 'n'])}\")\n",
    "print()\n",
    "print(f\"C={Counter(C)}\")\n",
    "print(f\"T={Counter(T)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9010419015831491\n",
      "F1: 0.8978291959432556\n",
      "\n",
      "cm:\n",
      "[[7504  568   28]\n",
      " [1120 5171   15]\n",
      " [ 195  268 7302]]\n",
      "\n",
      "C=Counter({'s': 8819, 'n': 7345, 'r': 6007})\n",
      "T=Counter({'s': 8100, 'n': 7765, 'r': 6306})\n"
     ]
    }
   ],
   "source": [
    "print(f\"acc: {accuracy_score(T, C)}\")\n",
    "print(f\"F1: {f1_score(T, C, average='macro')}\")\n",
    "# print(f\"cm:\\n{confusion_matrix(T, C)}\")\n",
    "print()\n",
    "print(f\"cm:\\n{confusion_matrix(T, C, labels=['s', 'r', 'n'])}\")\n",
    "print()\n",
    "print(f\"C={Counter(C)}\")\n",
    "print(f\"T={Counter(T)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_length = 128\n",
    "    claims = examples[\"claim\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    targets = examples[\"label\"]\n",
    "    inputs = [claim + \"</s>\" + context for claim, context in zip(claims, contexts)]\n",
    "    # inputs = [context + \"</s>\" + claim for claim, context in zip(claims, contexts)]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    # labels = tokenizer(targets, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    # labels = labels[\"input_ids\"]\n",
    "    # labels[labels == tokenizer.pad_token_id] = -100\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# tokenized_nli = raw_nli.map(preprocess_function, batched=True,  \n",
    "#                     #   remove_columns=raw_nli[\"train\"].column_names,\n",
    "#                       load_from_cache_file=False)\n",
    "# tokenized_nli.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "def split_predict(model, split, batch_size=256):\n",
    "    def predict_batch(inputs):\n",
    "        Y = model(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"].to(\"cuda\")).logits.argmax(dim=1)\n",
    "        C = [model.config.id2label[id_.item()] for id_ in Y]\n",
    "        out = {\"pred\": C}\n",
    "        return out\n",
    "    \n",
    "    split = split.map(predict_batch, batch_size=batch_size, batched=True)\n",
    "    C = split[\"pred\"]\n",
    "    T = tokenized_nli[\"dev\"][\"label\"]\n",
    "    return C, T\n",
    "\n",
    "def split_predict_generate(model, split, batch_size=256):\n",
    "    def predict_batch(inputs):\n",
    "        Y = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), attention_mask=inputs[\"attention_mask\"].to(\"cuda\"))\n",
    "        out = {\"pred\": tokenizer.batch_decode(Y, skip_special_tokens=True)}\n",
    "        return out\n",
    "    \n",
    "    split = split.map(predict_batch, batch_size=batch_size, batched=True)\n",
    "    C = split[\"pred\"]\n",
    "    T = tokenized_nli[\"dev\"][\"label\"]\n",
    "    return C, T\n",
    "\n",
    "# C, T = split_predict(model, tokenized_nli[\"dev\"], batch_size=64)\n",
    "# C, T = split_predict_generate(model, tokenized_nli[\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old type of models used in FactSearch\n",
    "from prediction.nli import SupportRefuteNEIModel # Make OBSOLETE\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "model_id = \"ctu-aic/xlm-roberta-large-xnli-csfever\"\n",
    "id2label = {0: \"s\", 1: \"r\", 2: \"n\"}\n",
    "label2id = {\"s\": 0, \"r\": 1, \"n\": 1}\n",
    "model = CrossEncoder(model_id, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "1280\n",
      "647\n",
      "acc: 0.446749709616694\n",
      "cm:\n",
      "[[   0    0    0]\n",
      " [2359 1251 8553]\n",
      " [2504  397 9903]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from aic_nlp_utils.batch import batch_apply\n",
    "\n",
    "def split_predict_generate(model, split, batch_size=128):\n",
    "    def predict(inputs):\n",
    "        X = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = X[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = X[\"attention_mask\"].to(\"cuda\")\n",
    "        Y = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        C = tokenizer.batch_decode(Y, skip_special_tokens=True)\n",
    "        return C\n",
    "\n",
    "def split_predict_crossencoder(model, split, batch_size=10*128):\n",
    "    def predict(inputs):\n",
    "        print(len(inputs))\n",
    "        Y = model.predict(inputs).argmax(axis=1)\n",
    "        C = [id2label[id_.item()] for id_ in Y]\n",
    "        return C\n",
    "        \n",
    "    # SWITCHED CTX and CLAIM!!!\n",
    "    inputs = [[context, claim] for claim, context in zip(split[\"claim\"],  split[\"context\"])]\n",
    "    C = batch_apply(predict, inputs, batch_size=batch_size)\n",
    "    T = [l for l in split[\"label\"]]\n",
    "    return C, T\n",
    "\n",
    "# C, T = split_predict_generate(model, raw_nli[\"dev\"])\n",
    "C, T = split_predict_crossencoder(model, raw_nli[\"dev\"])\n",
    "print(f\"acc: {accuracy_score(T, C)}\")\n",
    "print(f\"cm:\\n{confusion_matrix(T, C)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5060786993741457\n",
      "cm:\n",
      "[[4183   68  188]\n",
      " [2349  826 1184]\n",
      " [2531  546 2026]]\n"
     ]
    }
   ],
   "source": [
    "C, T = split_predict(model, raw_nli[\"test\"])\n",
    "print(f\"acc: {accuracy_score(T, C)}\")\n",
    "print(f\"cm:\\n{confusion_matrix(T, C)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.49263804969579006\n",
      "cm:\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     1     0     0     0     0     0 41839  1438  4126]\n",
      " [    1     0     0     1     0     0     0     0 19317  7391 17376]\n",
      " [    1     1     1     0     1     1     2     1 23257  6941 21133]]\n"
     ]
    }
   ],
   "source": [
    "C, T = split_predict(model, raw_nli[\"train\"])\n",
    "print(f\"acc: {accuracy_score(T, C)}\")\n",
    "print(f\"cm:\\n{confusion_matrix(T, C)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflarge",
   "language": "python",
   "name": "hflarge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
