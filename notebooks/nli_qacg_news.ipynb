{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "from itertools import chain\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import unicodedata as ud\n",
    "from time import time\n",
    "from typing import Dict, Type, Callable, List, Union\n",
    "import sys\n",
    "import ujson\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.fever import fever_detokenize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** move elsewhere NLI models should be covered in own package. Currently it is here for convenience only.\n",
    "\n",
    "This one is for CTK, cRO, Parlamentni Listy and DenikN\n",
    "\n",
    "Splits are done in ColBERT notebooks. See: `prepare_data_news.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH = \"full\" # all generated data\n",
    "# APPROACH = \"balanced\" # balanced classes\n",
    "APPROACH = \"balanced_shuf\" # balanced classes, shuffled\n",
    "# APPROACH = \"fever_size\" # QACG data subsampled to Cs/EnFEVER dataset size\n",
    "\n",
    "LANG = \"cs\"\n",
    "NER_DIR = \"PAV-ner-CNEC\"\n",
    "\n",
    "QG_DIR = \"mt5-large_all-cp126k\"\n",
    "QACG_DIR = \"mt5-large_all-cp156k\"\n",
    "\n",
    "# BELOW configuration is language-agnostic\n",
    "\n",
    "# DATA_ROOT = f\"/mnt/data/cro/factcheck/v1\"\n",
    "# DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"cro_paragraphs_filtered.jsonl\")\n",
    "\n",
    "# DATA_ROOT = f\"/mnt/data/ctknews/factcheck/par6\"\n",
    "# DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"jsonl\", \"ctk_filtered.jsonl\")\n",
    "\n",
    "# DATA_ROOT = f\"/mnt/data/factcheck/denikn/v1\"\n",
    "# DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"denikn_paragraphs.jsonl\")\n",
    "\n",
    "DATA_ROOT = f\"/mnt/data/newton/parlamentni_listy/factcheck/v1\"\n",
    "DATA_CORPUS = Path(DATA_ROOT, \"interim\", \"plisty_paragraphs.jsonl\")\n",
    "\n",
    "QACG_ROOT = Path(DATA_ROOT, \"qacg\")\n",
    "\n",
    "NLI_DIR = Path(\"nli\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "NLI_ROOT = Path(QACG_ROOT, NLI_DIR)\n",
    "\n",
    "SPLIT_DIR = Path(\"splits\", NER_DIR, QG_DIR, QACG_DIR)\n",
    "SPLIT_ROOT = Path(QACG_ROOT, SPLIT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPLIT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010813236236572266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a681309f6e412989fe750ca77d056d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def import_corpus(corpus_file):\n",
    "    # it already has correct format\n",
    "    raw = read_jsonl(corpus_file, show_progress=True)\n",
    "    for e in raw:\n",
    "        e[\"id\"] = nfc(e[\"id\"])\n",
    "        if \"did\" not in e:\n",
    "            did, bid = e[\"id\"].split(\"_\")\n",
    "            e[\"bid\"] = bid\n",
    "            e[\"did\"] = did\n",
    "        e[\"did\"] = nfc(str(e[\"did\"]))\n",
    "        e[\"text\"] = nfc(e[\"text\"])\n",
    "    return raw\n",
    "\n",
    "\n",
    "def generate_original_id2pid_mapping(corpus):\n",
    "    original_id2pid = {}\n",
    "    for pid, r in enumerate(corpus):\n",
    "        original_id = r[\"id\"]\n",
    "        # assert original_id not in original_id2pid, f\"original ID not unique! {original_id}\"\n",
    "        if original_id in original_id2pid:\n",
    "            print(f\"original ID not unique! {pid} {original_id}, previous pid: {original_id2pid[original_id]}\")\n",
    "        original_id2pid[original_id] = pid\n",
    "    return original_id2pid\n",
    "\n",
    "corpus = import_corpus(DATA_CORPUS)\n",
    "original_id2pid = generate_original_id2pid_mapping(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57938/57938 [00:00<00:00, 422070.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 57938, label counts: Counter({'n': 19477, 'r': 19393, 's': 19068}) to:\n",
      " /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/train_balanced_shuf.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5903/5903 [00:00<00:00, 521077.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 5903, label counts: Counter({'n': 1993, 'r': 1958, 's': 1952}) to:\n",
      " /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/dev_balanced_shuf.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5867/5867 [00:00<00:00, 573759.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 5867, label counts: Counter({'n': 1987, 'r': 1941, 's': 1939}) to:\n",
      " /mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/test_balanced_shuf.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_nli_data(src_file, dst_file, corpus, original_id2pid, seed=1234):\n",
    "    # imports data created for Evidence retrieval (ColBERTv2:prepare_data_wiki.ipynb)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    recs = []\n",
    "    counts = Counter()\n",
    "    data = read_jsonl(src_file)\n",
    "    for sample in tqdm(data):\n",
    "        claim = sample[\"claim\"]\n",
    "        label = sample[\"label\"]\n",
    "        evidence_bids = sample[\"evidence\"]\n",
    "        assert len(evidence_bids) == 1, \"More than single evidence not impemented (yet)\" \n",
    "        context = corpus[original_id2pid[evidence_bids[0]]][\"text\"]\n",
    "        recs.append({\"claim\": claim, \"context\": context, \"label\": label})\n",
    "        counts[label] += 1\n",
    "    rng.shuffle(recs)\n",
    "    print(f\"exporting {len(recs)}, label counts: {counts} to:\\n {str(dst_file)}\")\n",
    "    write_jsonl(dst_file, recs, mkdir=True)\n",
    "\n",
    "prepare_nli_data(Path(SPLIT_ROOT, f\"train_{APPROACH}.jsonl\"), Path(NLI_ROOT, f\"train_{APPROACH}.jsonl\"), corpus, original_id2pid, seed=1234)\n",
    "prepare_nli_data(Path(SPLIT_ROOT, f\"dev_{APPROACH}.jsonl\"), Path(NLI_ROOT, f\"dev_{APPROACH}.jsonl\"), corpus, original_id2pid, seed=1235)\n",
    "prepare_nli_data(Path(SPLIT_ROOT, f\"test_{APPROACH}.jsonl\"), Path(NLI_ROOT, f\"test_{APPROACH}.jsonl\"), corpus, original_id2pid, seed=1236)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum Sources\n",
    "Currently combines: CTK, cRO, PListy and DenikN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sum_split(src_files, dst_files, rng):\n",
    "    # the sum dataset simply concatenates (and shuffles) all source language datasets\n",
    "    data = [read_jsonl(src_file[1]) for src_file in src_files]\n",
    "    sources = [src_file[0] for src_file in src_files]\n",
    "    recs = []\n",
    "    for source, d in zip(sources, data):\n",
    "        indices = range(len(d))\n",
    "        rec = []\n",
    "        for idx in indices:\n",
    "            r = d[idx]\n",
    "            r[\"source\"] = source\n",
    "            r[\"orig_idx\"] = idx # the index in the original language claim file\n",
    "            rec.append(r)\n",
    "        recs += list(rec)\n",
    "    rng.shuffle(recs)\n",
    "    write_jsonl(dst_files, recs, mkdir=True)\n",
    "\n",
    "APPROACH = \"balanced_shuf\"\n",
    "rng = np.random.RandomState(1234)\n",
    "for split in [f\"train_{APPROACH}.jsonl\", f\"dev_{APPROACH}.jsonl\", f\"test_{APPROACH}.jsonl\", f\"train_{APPROACH}_no_nei.jsonl\", f\"dev_{APPROACH}_no_nei.jsonl\", f\"test_{APPROACH}_no_nei.jsonl\"]:\n",
    "    create_sum_split([\n",
    "        (\"cro\", Path(\"/mnt/data/cro/factcheck/v1/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"ctk\", Path(\"/mnt/data/ctknews/factcheck/par6/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"denikn\", Path(\"/mnt/data/factcheck/denikn/v1/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        (\"plisty\", Path(\"/mnt/data/newton/parlamentni_listy/factcheck/v1/qacg/splits/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k\", split)),\n",
    "        ],\n",
    "        Path(\"/mnt/data/factcheck/qacg/news_sum/qacg/splits\", split),\n",
    "        rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nli_data_combined(src_files, dst_files, src2fcorpus, seed=1234):\n",
    "    # imports data created for Evidence retrieval (ColBERTv2:prepare_data_news.ipynb)\n",
    "    srcs = [read_jsonl(src_file) for src_file in tqdm(src_files, desc=\"reading sources\")]\n",
    "\n",
    "    for source, fcorpus in src2fcorpus.items():\n",
    "        print(f\"loading corpus for {source.upper()} from '{fcorpus}'\")\n",
    "        corpus = import_corpus(fcorpus)\n",
    "        original_id2pid = generate_original_id2pid_mapping(corpus)\n",
    "        for src in srcs:\n",
    "            for sample in src:\n",
    "                if sample[\"source\"] == source:\n",
    "                    evidence_bids = sample[\"evidence\"]\n",
    "                    assert len(evidence_bids) == 1, \"More than single evidence not impemented (yet)\" \n",
    "                    context = corpus[original_id2pid[evidence_bids[0]]][\"text\"]\n",
    "                    sample[\"context\"] = context\n",
    "    for src, dst_file in zip(srcs, dst_files):\n",
    "        print(f\"exporting {len(src)} to:\\n {str(dst_file)}\")\n",
    "        write_jsonl(dst_file, src, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading sources: 100%|██████████| 3/3 [00:02<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for CRO from '/mnt/data/cro/factcheck/v1/interim/cro_paragraphs_filtered.jsonl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009356021881103516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc092ecef9f14db7bf65c1d600f83f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for CTK from '/mnt/data/ctknews/factcheck/par6/interim/jsonl/ctk_filtered.jsonl'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010849714279174805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4723c6fff84e0a8d08161760dcda41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for DENIKN from '/mnt/data/factcheck/denikn/v1/interim/denikn_paragraphs.jsonl'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009554147720336914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80d00f38dec424c8b58dab1b88fb174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading corpus for PLISTY from '/mnt/data/newton/parlamentni_listy/factcheck/v1/interim/plisty_paragraphs.jsonl'\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0809030532836914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d06bd9c61c40e99133010cc122fac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.00it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 27601 to:\n",
      " /mnt/data/factcheck/qacg/news_sum/qacg/nli/dev_balanced_shuf.jsonl\n",
      "exporting 27612 to:\n",
      " /mnt/data/factcheck/qacg/news_sum/qacg/nli/test_balanced_shuf.jsonl\n",
      "exporting 273300 to:\n",
      " /mnt/data/factcheck/qacg/news_sum/qacg/nli/train_balanced_shuf.jsonl\n"
     ]
    }
   ],
   "source": [
    "prepare_nli_data_combined(\n",
    "    src_files=[\n",
    "        f\"/mnt/data/factcheck/qacg/news_sum/qacg/splits/dev_balanced_shuf.jsonl\",\n",
    "        f\"/mnt/data/factcheck/qacg/news_sum/qacg/splits/test_balanced_shuf.jsonl\",\n",
    "        f\"/mnt/data/factcheck/qacg/news_sum/qacg/splits/train_balanced_shuf.jsonl\",\n",
    "    ],\n",
    "    dst_files=[\n",
    "        f\"/mnt/data/factcheck/qacg/news_sum/qacg/nli/dev_balanced_shuf.jsonl\",\n",
    "        f\"/mnt/data/factcheck/qacg/news_sum/qacg/nli/test_balanced_shuf.jsonl\",\n",
    "        f\"/mnt/data/factcheck/qacg/news_sum/qacg/nli/train_balanced_shuf.jsonl\",\n",
    "    ],\n",
    "    src2fcorpus={\n",
    "        \"cro\": \"/mnt/data/cro/factcheck/v1/interim/cro_paragraphs_filtered.jsonl\",\n",
    "        \"ctk\": \"/mnt/data/ctknews/factcheck/par6/interim/jsonl/ctk_filtered.jsonl\",\n",
    "        \"denikn\": \"/mnt/data/factcheck/denikn/v1/interim/denikn_paragraphs.jsonl\",\n",
    "        \"plisty\": \"/mnt/data/newton/parlamentni_listy/factcheck/v1/interim/plisty_paragraphs.jsonl\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflarge",
   "language": "python",
   "name": "hflarge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
