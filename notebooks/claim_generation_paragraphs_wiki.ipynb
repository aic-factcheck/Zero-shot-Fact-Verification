{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import ujson\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Set\n",
    "import unicodedata\n",
    "import uuid\n",
    "\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_jsonl, write_jsonl\n",
    "from aic_nlp_utils.fever import fever_detokenize\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "\n",
    "sys.path.append('Claim_Generation')\n",
    "from T5_QG import pipeline\n",
    "from distractor_generation import Distractor_Generation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is full version aimed at generating SUPPORTED and REFUTED claims needed for evidence retrieval, keeping NEIs for later.\n",
    "- It is an extended version of the `claim_generation.ipynb`.\n",
    "- Aimed to generate data for post LREV EnFEVER models (e.g., ColBERT v2).\n",
    "- The source data are based on **full** Wikipedia dump split to paragraphs. See `drchajan/notebooks/download_wiki.ipynb`\n",
    "- Only a sample of the full corpus is matched to pages appearing in the LREV EnFEVER so the generated claims (and models trained on them) are somewhat comparable to original EnFEVER dataset.\n",
    "- NEI context are retrieved from other paragraphs of the same Wikipedia page.\n",
    "- Fixed input and output formats for those we use in AIC.\n",
    "\n",
    "**Notes**\n",
    "- Currently ignoring multi-hops - single evidence documents are used only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR=\"/mnt/data/factcheck/claim_extraction/csfeversum/en/0.0.2\"\n",
    "LANG = \"en\"\n",
    "DATE = \"20230220\"\n",
    "WIKI_ROOT = f\"/mnt/data/factcheck/wiki/{LANG}/20230220\"\n",
    "QACG_ROOT = f\"{WIKI_ROOT}/qacg\"\n",
    "WIKI_CORPUS = f\"{WIKI_ROOT}/paragraphs/{LANG}wiki-20230220-paragraphs.jsonl\"\n",
    "FEVER_ROOT = \"/mnt/data/factcheck/fever/data-en-lrev/fever-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6204729"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = read_jsonl(WIKI_CORPUS)\n",
    "corpus_id2idx = {r[\"id\"]: i for i, r in enumerate(corpus)}\n",
    "corpus_pages = set(r[\"did\"] for r in corpus)\n",
    "len(corpus_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Anarchism_0',\n",
       " 'did': 'Anarchism',\n",
       " 'bid': 0,\n",
       " 'text': 'Anarchism',\n",
       " 'url': 'https://en.wikipedia.org/wiki?curid=12',\n",
       " 'revid': '6068332'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/factcheck/fever/data-en-lrev/fever-data/train.jsonl\n",
      "/mnt/data/factcheck/fever/data-en-lrev/fever-data/paper_dev.jsonl\n",
      "/mnt/data/factcheck/fever/data-en-lrev/fever-data/paper_test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12549, 1460, 1499)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_fever_evidence_pages(split_jsonls: List):\n",
    "    pages = set()\n",
    "    for jsonl in split_jsonls:\n",
    "        print(jsonl)\n",
    "        split = read_jsonl(jsonl)\n",
    "        for rec in split:\n",
    "            if rec[\"verifiable\"] == \"VERIFIABLE\":\n",
    "                for eset in rec[\"evidence\"]:\n",
    "                    for ev in eset:\n",
    "                        pages.add(ev[2])\n",
    "    return pages\n",
    "\n",
    "\n",
    "\n",
    "fever_pages_trn = extract_fever_evidence_pages([Path(FEVER_ROOT, \"train.jsonl\")])\n",
    "fever_pages_dev = extract_fever_evidence_pages([Path(FEVER_ROOT, \"paper_dev.jsonl\")])\n",
    "fever_pages_tst = extract_fever_evidence_pages([Path(FEVER_ROOT, \"paper_test.jsonl\")])\n",
    "len(fever_pages_trn), len(fever_pages_dev), len(fever_pages_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched 11247/12549 pages\n",
      "matched 1339/1460 pages\n",
      "matched 1358/1499 pages\n"
     ]
    }
   ],
   "source": [
    "# trying to match fever pages to corpus pages so we can generate claims based on topics comparable to EnFEVER\n",
    "# corpus is based on newer dump so the match can't be perfect\n",
    "def match_fever_to_corpus_pages(fever_pages: Set[str], corpus_pages: Set[str]):\n",
    "    fever_pages = set(fever_detokenize(p) for p in fever_pages)\n",
    "    matched = fever_pages.intersection(corpus_pages)\n",
    "    print(f\"matched {len(matched)}/{len(fever_pages)} pages\")\n",
    "    return matched\n",
    "\n",
    "sel_corpus_pages_trn = match_fever_to_corpus_pages(fever_pages_trn, corpus_pages)\n",
    "sel_corpus_pages_dev = match_fever_to_corpus_pages(fever_pages_dev, corpus_pages)\n",
    "sel_corpus_pages_tst = match_fever_to_corpus_pages(fever_pages_tst, corpus_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_pages(corpus, corpus_id2idx, sel_corpus_pages: Set[str]):\n",
    "    recs = []\n",
    "    for p in sel_corpus_pages:\n",
    "        corpus_rec = corpus[corpus_id2idx[p + \"_1\"]] # take the first paragraph - should roughly mimic the leadning parts from EnFEVER\n",
    "        recs.append(corpus_rec)\n",
    "    return recs\n",
    "    \n",
    "corpus_recs_trn = extract_corpus_pages(corpus, corpus_id2idx, sel_corpus_pages_trn)\n",
    "corpus_recs_dev = extract_corpus_pages(corpus, corpus_id2idx, sel_corpus_pages_dev)\n",
    "corpus_recs_tst = extract_corpus_pages(corpus, corpus_id2idx, sel_corpus_pages_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 12:35:46 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-04-03 12:35:46 INFO: Use device: gpu\n",
      "2023-04-03 12:35:46 INFO: Loading: tokenize\n",
      "2023-04-03 12:35:48 INFO: Loading: ner\n",
      "2023-04-03 12:35:49 INFO: Done loading processors!\n",
      "100%|██████████| 1339/1339 [00:53<00:00, 24.98it/s]\n",
      "2023-04-03 12:36:42 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-04-03 12:36:42 INFO: Use device: gpu\n",
      "2023-04-03 12:36:42 INFO: Loading: tokenize\n",
      "2023-04-03 12:36:42 INFO: Loading: ner\n",
      "2023-04-03 12:36:43 INFO: Done loading processors!\n",
      "100%|██████████| 1358/1358 [00:57<00:00, 23.58it/s]\n",
      "2023-04-03 12:37:40 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-04-03 12:37:40 INFO: Use device: gpu\n",
      "2023-04-03 12:37:40 INFO: Loading: tokenize\n",
      "2023-04-03 12:37:40 INFO: Loading: ner\n",
      "2023-04-03 12:37:41 INFO: Done loading processors!\n",
      "100%|██████████| 11247/11247 [07:27<00:00, 25.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_ners(corpus_recs, ner_json):\n",
    "    # for each text gives a triplet (ner, ner_type, ner-ner_type count in text)\n",
    "    # the triplets are sorted by decreasing count\n",
    "    stanza_nlp = stanza.Pipeline('en', use_gpu = True, processors=\"tokenize,ner\")\n",
    "    entity_dict = OrderedDict()\n",
    "    for l in tqdm(corpus_recs):\n",
    "        text = l[\"text\"]\n",
    "        pass_doc = stanza_nlp(text)\n",
    "        ner_pairs = [(ent.text, ent.type) for ent in pass_doc.ents] # text-type pairs\n",
    "        ner_cnts = Counter(ner_pairs) # their \n",
    "        ners_unique_with_counts =  [(p[0], p[1], ner_cnts[(p[0], p[1])]) for p in set(ner_pairs)]\n",
    "        ners_unique_with_counts = sorted(ners_unique_with_counts, key=lambda n: -n[2])\n",
    "        entity_dict[l[\"id\"]] = ners_unique_with_counts\n",
    "    Path(ner_json).parent.mkdir(parents=True, exist_ok=True)    \n",
    "    write_json(ner_json, entity_dict)\n",
    "\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"dev_ners.json\"))\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"test_ners.json\"))\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"train_ners.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QG module >>>>>>>>\n",
      "QG module loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1358/1358 [14:21<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#invalid samples: 51\n",
      "Loading QG module >>>>>>>>\n",
      "QG module loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11247/11247 [1:59:33<00:00,  1.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#invalid samples: 373\n"
     ]
    }
   ],
   "source": [
    "def generate_qas(corpus_recs, ner_json, qas_json):\n",
    "    # QG NLP object\n",
    "    gpu_index = 0\n",
    "\n",
    "    print('Loading QG module >>>>>>>>')\n",
    "    qg_nlp = pipeline(\"question-generation\", model='valhalla/t5-base-qg-hl', qg_format=\"highlight\", gpu_index = gpu_index)\n",
    "    print('QG module loaded.')\n",
    "\n",
    "    ners = read_json(ner_json)\n",
    "\n",
    "    qas = OrderedDict()\n",
    "    invalid_sample = 0\n",
    "    for l in tqdm(corpus_recs):\n",
    "        entities = ners[str(l['id'])]\n",
    "\n",
    "        # create a batch\n",
    "        sources, answers = [], []\n",
    "        for ent_text, ent_type, ent_cnt in entities:\n",
    "            sources.append(l['text'])\n",
    "            answers.append(ent_text)\n",
    "            \n",
    "        # question generation\n",
    "        if len(sources) > 0 and len(sources) == len(answers):\n",
    "            results = []\n",
    "            try:\n",
    "                results = qg_nlp.batch_qg_with_answer(sources, answers)\n",
    "            except:\n",
    "                invalid_sample += 1\n",
    "\n",
    "            if len(results) == 0:\n",
    "                continue\n",
    "            \n",
    "            # save results\n",
    "            result_for_sample = {}\n",
    "            for ind, QA in enumerate(results):\n",
    "                ent_text, ent_type, _ = entities[ind]\n",
    "                question = QA['question']\n",
    "                answer = QA['answer']\n",
    "                result_for_sample[f'{ent_text}:::{ent_type}'] = [question, answer]\n",
    "\n",
    "            qas[str(l['id'])] = result_for_sample\n",
    "        else:\n",
    "            invalid_sample += 1\n",
    "\n",
    "    print(f'#invalid samples: {invalid_sample}')\n",
    "    Path(qas_json).parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_json(qas_json, qas)\n",
    "\n",
    "\n",
    "generate_qas(corpus_recs_dev, Path(QACG_ROOT, \"dev_ners.json\"), Path(QACG_ROOT, \"dev_qas.json\"))\n",
    "generate_qas(corpus_recs_tst, Path(QACG_ROOT, \"test_ners.json\"), Path(QACG_ROOT, \"test_qas.json\"))\n",
    "generate_qas(corpus_recs_trn, Path(QACG_ROOT, \"train_ners.json\"), Path(QACG_ROOT, \"train_qas.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimGenerator:\n",
    "    def __init__(self, corpus_recs, ner_json, qas_json, QA2D_model_path, sense_to_vec_path, gpu_index=0):\n",
    "        # QA2D model object\n",
    "        print('Loading QA2D module >>>>>>>>')\n",
    "        model_args = Seq2SeqArgs()\n",
    "        model_args.max_length = 64\n",
    "        model_args.silent = True\n",
    "\n",
    "        self.QA2D_model = Seq2SeqModel(\n",
    "            encoder_decoder_type=\"bart\", \n",
    "            encoder_decoder_name=QA2D_model_path,\n",
    "            cuda_device=gpu_index,\n",
    "            args=model_args\n",
    "        )\n",
    "\n",
    "        print('Loading Replacement Generator module >>>>>>>>')\n",
    "        self.replacement_generator = Distractor_Generation(sense2vec_path = sense_to_vec_path, T = 0.7)\n",
    "\n",
    "        self.corpus_recs = corpus_recs\n",
    "        self.ners = read_json(ner_json)\n",
    "        self.qas = read_json(qas_json)\n",
    "\n",
    "\n",
    "    def _load_passage_entities(self, id_):\n",
    "        passage_entities = []\n",
    "        for ent_text, ent_type, _ in self.ners[id_]:\n",
    "            passage_entities.append(f'{ent_text}:::{ent_type}') # group by entity name and type as in the QAS file\n",
    "        return passage_entities\n",
    "    \n",
    "    def _load_precomputed_qas_for_entities(self, id_, passage_entities):\n",
    "        if id_ not in self.qas:\n",
    "            print(f\"missing id: {id_}\")\n",
    "            return None\n",
    "        QA_for_sample = self.qas[id_]\n",
    "        QA_pairs = []\n",
    "        for entity in passage_entities:\n",
    "            if entity in QA_for_sample:\n",
    "                ent_text, ent_type = entity.split(':::')\n",
    "                question, answer = QA_for_sample[entity]\n",
    "                QA_pairs.append({'question': question, 'answer': answer, 'answer_type': ent_type})\n",
    "            else:\n",
    "                print(f\"missing entity: {entity} for id: {id_}\")\n",
    "                return None\n",
    "        if len(QA_pairs) == 0:\n",
    "            print(f\"zero length pairs for id: {id_}\")\n",
    "            return None\n",
    "        return QA_pairs \n",
    "        \n",
    "\n",
    "    def generate_supported_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "\n",
    "        # Step 2: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 3: QA2D\n",
    "        to_predict = [qa['question'] + ' [SEP] ' + qa['answer'] for qa in QA_pairs]\n",
    "        results = []\n",
    "        # try:\n",
    "        results = self.QA2D_model.predict(to_predict)\n",
    "        # except:\n",
    "            # return None\n",
    "        if len(results) == 0:\n",
    "            print(f\"zero length results for id: {id_}\")\n",
    "            return None\n",
    "\n",
    "        assert len(results) == len(QA_pairs)\n",
    "\n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(passage_entities, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "    def generate_refute_global_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "        \n",
    "        # Step 2: get entity replacement\n",
    "        entity_replacement_dict = {} # get replacement beforehand to save time\n",
    "\n",
    "        valid_entities = set()\n",
    "        for ent in passage_entities:\n",
    "            ent_text, _ = ent.split(':::')\n",
    "            replacement = self.replacement_generator.get_options(ent_text)\n",
    "            if replacement is not None:\n",
    "                entity_replacement_dict[ent_text] = replacement\n",
    "                valid_entities.add(ent)\n",
    "\n",
    "        # Step 3: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 4: Answer Replacement\n",
    "        to_predict = []\n",
    "        replace_type = []\n",
    "        replace_keys = []\n",
    "        for qa in QA_pairs:\n",
    "            ans_ent_text = qa['answer']\n",
    "            ans_ent_type = qa['answer_type']\n",
    "            if ans_ent_text == \"\" or ans_ent_type == \"\":\n",
    "                continue\n",
    "            replacement = entity_replacement_dict.get(ans_ent_text)\n",
    "            if replacement is not None:\n",
    "                to_predict.append(qa['question'] + ' [SEP] ' + replacement[0])\n",
    "                replace_keys.append(f\"{ans_ent_text}:::{ans_ent_type}\")\n",
    "                replace_type.append(ans_ent_type)\n",
    "        \n",
    "        # Step 5: QA2D\n",
    "        if len(to_predict) == 0:\n",
    "            return None\n",
    "        results = []\n",
    "        try:\n",
    "            results = self.QA2D_model.predict(to_predict)\n",
    "        except:\n",
    "            return None\n",
    "        if len(results) == 0:\n",
    "            return None\n",
    "        \n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(replace_keys, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "\n",
    "    def generate(self, claims_json, claim_type: str):\n",
    "        claim_type = claim_type.lower()\n",
    "        assert claim_type in [\"supported\", \"refuted\"]\n",
    "        generated_claims = OrderedDict()\n",
    "        for sample in tqdm(self.corpus_recs):\n",
    "            id_ = str(sample['id'])\n",
    "            if claim_type == \"supported\":\n",
    "                claims = self.generate_supported_claims(sample)\n",
    "            elif claim_type == \"refuted\":\n",
    "                claims = self.generate_refute_global_claims(sample)\n",
    "            if claims is None:\n",
    "                claims = {}\n",
    "            generated_claims[id_] = claims\n",
    "\n",
    "        write_json(claims_json, generated_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Loading Replacement Generator module >>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [1:50:44<00:00,  4.96s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Loading Replacement Generator module >>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1358/1358 [1:50:21<00:00,  4.88s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Loading Replacement Generator module >>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11247/11247 [15:36:13<00:00,  4.99s/it]  \n"
     ]
    }
   ],
   "source": [
    "confs = [\n",
    "    (corpus_recs_dev, \"dev\"),\n",
    "    (corpus_recs_tst, \"test\"),\n",
    "    (corpus_recs_trn, \"train\"),\n",
    "]\n",
    "\n",
    "for corpus_recs, name in confs:\n",
    "    claim_generator = ClaimGenerator(corpus_recs, \n",
    "                                 ner_json=Path(QACG_ROOT, f\"{name}_ners.json\"), \n",
    "                                 qas_json=Path(QACG_ROOT, f\"{name}_qas.json\"), \n",
    "                                 QA2D_model_path=\"dependencies/QA2D_model\", \n",
    "                                 sense_to_vec_path=\"dependencies/s2v_old\")\n",
    "\n",
    "    claim_generator.generate(Path(QACG_ROOT, f\"{name}_sup_claims.json\"), \"supported\")\n",
    "    claim_generator.generate(Path(QACG_ROOT, f\"{name}_ref_claims.json\"), \"refuted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_generator = Distractor_Generation(sense2vec_path=\"dependencies/s2v_old/\", T=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('American colonists', 'NOUN')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_generator.get_options(\"King George\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env_plight_env",
   "language": "python",
   "name": "fc_env_plight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
