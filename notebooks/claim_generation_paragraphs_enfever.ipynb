{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import ujson\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Set\n",
    "import unicodedata\n",
    "import uuid\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.fever import fever_detokenize, import_fever_corpus_from_sqlite\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "\n",
    "sys.path.append('Claim_Generation')\n",
    "from T5_QG import pipeline\n",
    "from distractor_generation import Distractor_Generation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is full version aimed at generating SUPPORTED and REFUTED claims needed for evidence retrieval, keeping NEIs for later.\n",
    "- It is a simplified version of the `claim_generation_paragraphs_wiki.ipynb`.\n",
    "- Aimed to generate data for post LREV EnFEVER models (e.g., ColBERT v2) for LREV EnFEVER corpus.\n",
    "- Fixed input and output formats for those we use in AIC.\n",
    "\n",
    "**Notes**\n",
    "- Currently ignoring multi-hops - single evidence documents are used only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR=\"/mnt/data/factcheck/claim_extraction/csfeversum/en/0.0.2\"\n",
    "FEVER_ROOT = \"/mnt/data/factcheck/fever/data-en-lrev/fever-data\"\n",
    "FEVER_CORPUS_SQLITE = \"/mnt/data/factcheck/fever/data-en-lrev/fever/fever.db\"\n",
    "QACG_ROOT = \"/mnt/data/factcheck/fever/data-en-lrev/qacg\"\n",
    "Path(QACG_ROOT).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396106"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = import_fever_corpus_from_sqlite(FEVER_CORPUS_SQLITE)\n",
    "corpus_id2idx = {r[\"id\"]: i for i, r in enumerate(corpus)}\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1928_in_association_football',\n",
       " 'text': 'The following are the football (soccer) events of the year 1928 throughout the world.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/factcheck/fever/data-en-lrev/fever-data/train.jsonl\n",
      "/mnt/data/factcheck/fever/data-en-lrev/fever-data/paper_dev.jsonl\n",
      "/mnt/data/factcheck/fever/data-en-lrev/fever-data/paper_test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12549, 1460, 1499)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_fever_evidence_pages(split_jsonls: List):\n",
    "    pages = set()\n",
    "    for jsonl in split_jsonls:\n",
    "        print(jsonl)\n",
    "        split = read_jsonl(jsonl)\n",
    "        for rec in split:\n",
    "            if rec[\"verifiable\"] == \"VERIFIABLE\":\n",
    "                for eset in rec[\"evidence\"]:\n",
    "                    for ev in eset:\n",
    "                        pages.add(ev[2])\n",
    "    return pages\n",
    "\n",
    "\n",
    "\n",
    "fever_pages_trn = extract_fever_evidence_pages([Path(FEVER_ROOT, \"train.jsonl\")])\n",
    "fever_pages_dev = extract_fever_evidence_pages([Path(FEVER_ROOT, \"paper_dev.jsonl\")])\n",
    "fever_pages_tst = extract_fever_evidence_pages([Path(FEVER_ROOT, \"paper_test.jsonl\")])\n",
    "len(fever_pages_trn), len(fever_pages_dev), len(fever_pages_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_pages(corpus, corpus_id2idx, sel_corpus_pages: Set[str]):\n",
    "    recs = []\n",
    "    for p in sel_corpus_pages:\n",
    "        corpus_rec = corpus[corpus_id2idx[p]]\n",
    "        recs.append(corpus_rec)\n",
    "    return recs\n",
    "    \n",
    "corpus_recs_trn = extract_corpus_pages(corpus, corpus_id2idx, fever_pages_trn)\n",
    "corpus_recs_dev = extract_corpus_pages(corpus, corpus_id2idx, fever_pages_dev)\n",
    "corpus_recs_tst = extract_corpus_pages(corpus, corpus_id2idx, fever_pages_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'A_Thousand_Suns',\n",
       " 'text': 'A Thousand Suns is the fourth studio album by American rock band Linkin Park. It was released on September 8, 2010, by Warner Bros.. Records. The album was written by the band and was produced by Linkin Park vocalist Mike Shinoda and Rick Rubin, who worked together to produce the band\\'s previous studio album Minutes to Midnight (2007). Recording sessions for A Thousand Suns took place at NRG Recording Studios in North Hollywood, California from 2008 until early 2010.   A Thousand Suns is a multi-concept album dealing with human fears such as nuclear warfare. The band has said the album is a drastic departure from their previous work; they experimented on different and new sounds. Shinoda told MTV the album references numerous social issues and blends human ideas with technology. The title is a reference to Hindu Sanskrit scripture, a line of which was first popularized in 1945 by J. Robert Oppenheimer, who described the atomic bomb as being \"as bright as a thousand suns\". It also appears in a line from the first single of the album, \"The Catalyst\".   \"The Catalyst\" was sent to radio and released to digital music retailers on August 2, 2010. \"The Catalyst\" peaked at the Billboard Alternative Songs and Rock Songs charts. Three more singles were released to promote the album: \"Waiting for the End\", \"Burning in the Skies\" and \"Iridescent\". \"The Catalyst\" and \"Waiting for the End\" were certified gold by the Recording Industry Association of America (RIAA). Linkin Park promoted the album through the A Thousand Suns World Tour from October 2010 to September 2011.   Upon release, the album polarized critics and fans. The band\\'s fanbase divided over their new sound, splitting them into \"love-it versus hate-it groups\" according to one reviewer. Despite this, the album has been a commercial success, debuting at number one on over ten charts. It was certified gold by the RIAA in February 2011. By June 2014, it had sold over 960,000 copies in the United States according to Nielsen SoundScan.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_recs_trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 13:06:20 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-04-05 13:06:20 INFO: Use device: cpu\n",
      "2023-04-05 13:06:20 INFO: Loading: tokenize\n",
      "2023-04-05 13:06:20 INFO: Loading: ner\n",
      "2023-04-05 13:06:23 INFO: Done loading processors!\n",
      "100%|██████████| 1460/1460 [21:28<00:00,  1.13it/s]\n",
      "2023-04-05 13:27:52 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-04-05 13:27:52 INFO: Use device: cpu\n",
      "2023-04-05 13:27:52 INFO: Loading: tokenize\n",
      "2023-04-05 13:27:52 INFO: Loading: ner\n",
      "2023-04-05 13:27:52 INFO: Done loading processors!\n",
      "100%|██████████| 1499/1499 [22:33<00:00,  1.11it/s]\n",
      "2023-04-05 13:50:25 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-04-05 13:50:25 INFO: Use device: cpu\n",
      "2023-04-05 13:50:25 INFO: Loading: tokenize\n",
      "2023-04-05 13:50:25 INFO: Loading: ner\n",
      "2023-04-05 13:50:26 INFO: Done loading processors!\n",
      "100%|██████████| 12549/12549 [2:52:53<00:00,  1.21it/s]  \n"
     ]
    }
   ],
   "source": [
    "def extract_ners(corpus_recs, ner_json):\n",
    "    # for each text gives a triplet (ner, ner_type, ner-ner_type count in text)\n",
    "    # the triplets are sorted by decreasing count\n",
    "    stanza_nlp = stanza.Pipeline('en', use_gpu = True, processors=\"tokenize,ner\")\n",
    "    entity_dict = OrderedDict()\n",
    "    for l in tqdm(corpus_recs):\n",
    "        text = l[\"text\"]\n",
    "        pass_doc = stanza_nlp(text)\n",
    "        ner_pairs = [(ent.text, ent.type) for ent in pass_doc.ents] # text-type pairs\n",
    "        ner_cnts = Counter(ner_pairs) # their \n",
    "        ners_unique_with_counts =  [(p[0], p[1], ner_cnts[(p[0], p[1])]) for p in set(ner_pairs)]\n",
    "        ners_unique_with_counts = sorted(ners_unique_with_counts, key=lambda n: -n[2])\n",
    "        entity_dict[l[\"id\"]] = ners_unique_with_counts\n",
    "    write_json(ner_json, entity_dict)\n",
    "\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"dev_ners.json\"))\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"test_ners.json\"))\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"train_ners.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QG module >>>>>>>>\n",
      "QG module loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460/1460 [27:05<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#invalid samples: 20\n",
      "Loading QG module >>>>>>>>\n",
      "QG module loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1499/1499 [29:09<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#invalid samples: 24\n",
      "Loading QG module >>>>>>>>\n",
      "QG module loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12549/12549 [3:45:08<00:00,  1.08s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#invalid samples: 208\n"
     ]
    }
   ],
   "source": [
    "def generate_qas(corpus_recs, ner_json, qas_json):\n",
    "    # QG NLP object\n",
    "    gpu_index = 0\n",
    "\n",
    "    print('Loading QG module >>>>>>>>')\n",
    "    qg_nlp = pipeline(\"question-generation\", model='valhalla/t5-base-qg-hl', qg_format=\"highlight\", gpu_index = gpu_index)\n",
    "    print('QG module loaded.')\n",
    "\n",
    "    ners = read_json(ner_json)\n",
    "\n",
    "    qas = OrderedDict()\n",
    "    invalid_sample = 0\n",
    "    for l in tqdm(corpus_recs):\n",
    "        entities = ners[str(l['id'])]\n",
    "\n",
    "        # create a batch\n",
    "        sources, answers = [], []\n",
    "        for ent_text, ent_type, ent_cnt in entities:\n",
    "            sources.append(l['text'])\n",
    "            answers.append(ent_text)\n",
    "            \n",
    "        # question generation\n",
    "        if len(sources) > 0 and len(sources) == len(answers):\n",
    "            results = []\n",
    "            try:\n",
    "                results = qg_nlp.batch_qg_with_answer(sources, answers)\n",
    "            except:\n",
    "                invalid_sample += 1\n",
    "\n",
    "            if len(results) == 0:\n",
    "                continue\n",
    "            \n",
    "            # save results\n",
    "            result_for_sample = {}\n",
    "            for ind, QA in enumerate(results):\n",
    "                ent_text, ent_type, _ = entities[ind]\n",
    "                question = QA['question']\n",
    "                answer = QA['answer']\n",
    "                result_for_sample[f'{ent_text}:::{ent_type}'] = [question, answer]\n",
    "\n",
    "            qas[str(l['id'])] = result_for_sample\n",
    "        else:\n",
    "            invalid_sample += 1\n",
    "\n",
    "    print(f'#invalid samples: {invalid_sample}')\n",
    "    Path(qas_json).parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_json(qas_json, qas)\n",
    "\n",
    "\n",
    "generate_qas(corpus_recs_dev, Path(QACG_ROOT, \"dev_ners.json\"), Path(QACG_ROOT, \"dev_qas.json\"))\n",
    "generate_qas(corpus_recs_tst, Path(QACG_ROOT, \"test_ners.json\"), Path(QACG_ROOT, \"test_qas.json\"))\n",
    "generate_qas(corpus_recs_trn, Path(QACG_ROOT, \"train_ners.json\"), Path(QACG_ROOT, \"train_qas.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimGenerator:\n",
    "    def __init__(self, corpus_recs, ner_json, qas_json, QA2D_model_path, sense_to_vec_path, gpu_index=0):\n",
    "        # QA2D model object\n",
    "        print('Loading QA2D module >>>>>>>>')\n",
    "        model_args = Seq2SeqArgs()\n",
    "        model_args.max_length = 64\n",
    "        model_args.silent = True\n",
    "\n",
    "        self.QA2D_model = Seq2SeqModel(\n",
    "            encoder_decoder_type=\"bart\", \n",
    "            encoder_decoder_name=QA2D_model_path,\n",
    "            cuda_device=gpu_index,\n",
    "            args=model_args\n",
    "        )\n",
    "\n",
    "        print('Loading Replacement Generator module >>>>>>>>')\n",
    "        self.replacement_generator = Distractor_Generation(sense2vec_path = sense_to_vec_path, T = 0.7)\n",
    "\n",
    "        self.corpus_recs = corpus_recs\n",
    "        self.ners = read_json(ner_json)\n",
    "        self.qas = read_json(qas_json)\n",
    "\n",
    "\n",
    "    def _load_passage_entities(self, id_):\n",
    "        passage_entities = []\n",
    "        for ent_text, ent_type, _ in self.ners[id_]:\n",
    "            passage_entities.append(f'{ent_text}:::{ent_type}') # group by entity name and type as in the QAS file\n",
    "        return passage_entities\n",
    "    \n",
    "    def _load_precomputed_qas_for_entities(self, id_, passage_entities):\n",
    "        if id_ not in self.qas:\n",
    "            print(f\"missing id: {id_}\")\n",
    "            return None\n",
    "        QA_for_sample = self.qas[id_]\n",
    "        QA_pairs = []\n",
    "        for entity in passage_entities:\n",
    "            if entity in QA_for_sample:\n",
    "                ent_text, ent_type = entity.split(':::')\n",
    "                question, answer = QA_for_sample[entity]\n",
    "                QA_pairs.append({'question': question, 'answer': answer, 'answer_type': ent_type})\n",
    "            else:\n",
    "                print(f\"missing entity: {entity} for id: {id_}\")\n",
    "                return None\n",
    "        if len(QA_pairs) == 0:\n",
    "            print(f\"zero length pairs for id: {id_}\")\n",
    "            return None\n",
    "        return QA_pairs \n",
    "        \n",
    "\n",
    "    def generate_supported_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "\n",
    "        # Step 2: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 3: QA2D\n",
    "        to_predict = [qa['question'] + ' [SEP] ' + qa['answer'] for qa in QA_pairs]\n",
    "        results = []\n",
    "        # try:\n",
    "        results = self.QA2D_model.predict(to_predict)\n",
    "        # except:\n",
    "            # return None\n",
    "        if len(results) == 0:\n",
    "            print(f\"zero length results for id: {id_}\")\n",
    "            return None\n",
    "\n",
    "        assert len(results) == len(QA_pairs)\n",
    "\n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(passage_entities, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "    def generate_refute_global_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "        \n",
    "        # Step 2: get entity replacement\n",
    "        entity_replacement_dict = {} # get replacement beforehand to save time\n",
    "\n",
    "        valid_entities = set()\n",
    "        for ent in passage_entities:\n",
    "            ent_text, _ = ent.split(':::')\n",
    "            replacement = self.replacement_generator.get_options(ent_text)\n",
    "            if replacement is not None:\n",
    "                entity_replacement_dict[ent_text] = replacement\n",
    "                valid_entities.add(ent)\n",
    "\n",
    "        # Step 3: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 4: Answer Replacement\n",
    "        to_predict = []\n",
    "        replace_type = []\n",
    "        replace_keys = []\n",
    "        for qa in QA_pairs:\n",
    "            ans_ent_text = qa['answer']\n",
    "            ans_ent_type = qa['answer_type']\n",
    "            if ans_ent_text == \"\" or ans_ent_type == \"\":\n",
    "                continue\n",
    "            replacement = entity_replacement_dict.get(ans_ent_text)\n",
    "            if replacement is not None:\n",
    "                to_predict.append(qa['question'] + ' [SEP] ' + replacement[0])\n",
    "                replace_keys.append(f\"{ans_ent_text}:::{ans_ent_type}\")\n",
    "                replace_type.append(ans_ent_type)\n",
    "        \n",
    "        # Step 5: QA2D\n",
    "        if len(to_predict) == 0:\n",
    "            return None\n",
    "        results = []\n",
    "        try:\n",
    "            results = self.QA2D_model.predict(to_predict)\n",
    "        except:\n",
    "            return None\n",
    "        if len(results) == 0:\n",
    "            return None\n",
    "        \n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(replace_keys, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "\n",
    "    def generate(self, claims_json, claim_type: str, save_every=0, cont=False):\n",
    "        claim_type = claim_type.lower()\n",
    "        assert claim_type in [\"supported\", \"refuted\"]\n",
    "        start = 0\n",
    "        if Path(claims_json).is_file():\n",
    "            if cont:\n",
    "                generated_claims = read_json(claims_json)\n",
    "                print(f\"file exists: {claims_json}, completed: {len(generated_claims)-1}/{len(self.corpus_recs)}\")\n",
    "                start = len(generated_claims)\n",
    "            else:\n",
    "                raise FileExistsError(f\"File already exists: {claims_json} !!!\")\n",
    "        else:\n",
    "            generated_claims = dict() # ordered since P3.7\n",
    "        cnt = 1\n",
    "        for sample in tqdm(self.corpus_recs[start:], initial=start, total=len(self.corpus_recs)):\n",
    "            id_ = str(sample['id'])\n",
    "            if claim_type == \"supported\":\n",
    "                claims = self.generate_supported_claims(sample)\n",
    "            elif claim_type == \"refuted\":\n",
    "                claims = self.generate_refute_global_claims(sample)\n",
    "            if claims is None:\n",
    "                claims = {}\n",
    "            generated_claims[id_] = claims\n",
    "            cnt += 1\n",
    "            if save_every > 0 and cnt % save_every == 0:\n",
    "                write_json(claims_json, generated_claims)\n",
    "\n",
    "        write_json(claims_json, generated_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Loading Replacement Generator module >>>>>>>>\n",
      "file exists: /mnt/data/factcheck/fever/data-en-lrev/qacg/train_ref_claims.json, completed: 8499/12549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8504it [00:59,  9.71s/it]"
     ]
    }
   ],
   "source": [
    "confs = [\n",
    "    # (corpus_recs_dev, \"dev\"),\n",
    "    # (corpus_recs_tst, \"test\"),\n",
    "    (corpus_recs_trn, \"train\"),\n",
    "]\n",
    "\n",
    "for corpus_recs, name in confs:\n",
    "    claim_generator = ClaimGenerator(corpus_recs, \n",
    "                                 ner_json=Path(QACG_ROOT, f\"{name}_ners.json\"), \n",
    "                                 qas_json=Path(QACG_ROOT, f\"{name}_qas.json\"), \n",
    "                                 QA2D_model_path=\"dependencies/QA2D_model\", \n",
    "                                 sense_to_vec_path=\"dependencies/s2v_old\")\n",
    "\n",
    "    # claim_generator.generate(Path(QACG_ROOT, f\"{name}_sup_claims.json\"), \"supported\", save_every=100)\n",
    "    claim_generator.generate(Path(QACG_ROOT, f\"{name}_ref_claims.json\"), \"refuted\", save_every=100, cont=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/factcheck/fever/data-en-lrev/qacg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QACG_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('American colonists', 'NOUN')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_generator.get_options(\"King George\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env_plight_env",
   "language": "python",
   "name": "fc_env_plight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
