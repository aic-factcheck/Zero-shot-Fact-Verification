{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter, OrderedDict\n",
    "import ujson\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import sys\n",
    "import textwrap\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Set, Union\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from aic_nlp_utils.batch import batch_apply\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.fever import fever_detokenize, import_fever_corpus_from_sqlite\n",
    "\n",
    "# from zshot_fact_verify.qa2d.qa2d import SameDocumentNERReplacementGenerator\n",
    "from zshot_fact_verify.wiki.load import load_corpus, create_corpus_splits, select_nei_context_for_splits, load_nei_ners\n",
    "# from zshot_fact_verify.models.load import load_tokenizer_and_model\n",
    "# from zshot_fact_verify.models.arguments import ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported 514568 corpus pages.\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "NER_ROOT = '/mnt/data/factcheck/wiki/cs/20230220/qacg/ner/PAV-ner-CNEC'\n",
    "WIKI_CORPUS = '/mnt/data/factcheck/wiki/cs/20230220/paragraphs/cswiki-20230220-paragraphs.jsonl'\n",
    "SPLITS  = [\n",
    "            {\"name\": \"train\", \"file\": Path(NER_ROOT, \"train_ners.json\"), \"size\": 10000},\n",
    "            {\"name\": \"dev\", \"file\": Path(NER_ROOT, \"dev_ners.json\"), \"size\": 1000},\n",
    "            {\"name\": \"test\", \"file\": Path(NER_ROOT, \"test_ners.json\"), \"size\": 1000},\n",
    "        ]\n",
    "corpus, corpus_id2idx, corpus_pages = load_corpus(WIKI_CORPUS)\n",
    "corpus_recs_lst = create_corpus_splits(corpus, corpus_id2idx, SPLITS, SEED)\n",
    "corpus_recs_lst = select_nei_context_for_splits(corpus, corpus_id2idx, corpus_recs_lst, SEED)\n",
    "corpus_recs_dict = {}\n",
    "for i, split in enumerate([\"train\", \"dev\", \"test\"]):\n",
    "    crl = corpus_recs_lst[i]\n",
    "    cd = {s[\"id\"]: {\"text\": s[\"text\"], \"nei_text\": s.get(\"nei_text\")} for s in crl}\n",
    "    corpus_recs_dict[split] = cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Martin Havelka',\n",
       " 'nei_text': 'Osobní život.\\nByl ženatý. Se svou manželkou Ivou Havelkovou měl tři děti: Jana, Martina a Emílii Emmu. Mezi jeho zájmy patřilo vyřezávání dřevěných soch, vše okolo indiánů, fotografování a také ho bavil westernový život. Žil nedaleko Brna ve vesnici jménem Radostice. Manželka pracuje také v Městském divadle Brno jako rekvizitářka a jeho syn Jan pracuje tamtéž jako osvětlovač.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_recs_dict[\"train\"]['Martin_Havelka_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/mnt/data/factcheck/wiki/cs/20230220/qacg'\n",
    "NER_DIR = 'PAV-ner-CNEC'\n",
    "QG_DIR = 'mt5-large-cp59k'\n",
    "QA2D_DIR = 'mbart-large-cc25_cp26k'\n",
    "\n",
    "CLAIM_ROOT = Path(DATA_ROOT, 'claim', NER_DIR, QG_DIR, QA2D_DIR)\n",
    "CLAIM_QUALITY_ROOT = Path(DATA_ROOT, 'claim_quality_v3', NER_DIR, QG_DIR, QA2D_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_claims(claim_root, corpus_recs_dict, split, claim_types, n_per_split, seed=1234):\n",
    "    # samples claims to be annotated in Doccano locally\n",
    "    # run: doccano task\n",
    "    # and: doccano webserver --port 8000 in /Users/drchajan/Downloads/doccano\n",
    "    corpus_recs = corpus_recs_dict[split]\n",
    "    rng = np.random.RandomState(seed)\n",
    "    sel = []\n",
    "    for ct in claim_types:\n",
    "        claim_dict = read_json(Path(claim_root, f\"{split}_{ct}.json\"))\n",
    "        claim_set = set()\n",
    "        claims = []\n",
    "        for pid_, page in claim_dict.items():\n",
    "            for id_, claim in page.items():\n",
    "                if claim not in claim_set:\n",
    "                    claim_set.add(claim)\n",
    "                    claims.append((pid_, claim))\n",
    "\n",
    "        claims = [claims[i] for i in rng.choice(len(claims), n_per_split, replace=False)]\n",
    "        for id_, c in claims:\n",
    "            meta = {\"claim_type\": ct, \"text\": corpus_recs[id_][\"text\"]}\n",
    "            if ct == \"nei\":\n",
    "                meta[\"nei_text\"] = corpus_recs[id_][\"nei_text\"]\n",
    "            sel.append({\"text\": c, \"meta\": meta})\n",
    "    rng.shuffle(sel)\n",
    "    return sel\n",
    "\n",
    "\n",
    "# claims = sample_random_claims(CLAIM_ROOT, corpus_recs_dict, \"train\", [\"support\", \"refute\", \"nei\"], n_per_split=1000)\n",
    "# write_jsonl(\"/home/drchajan/claim_quality-mbart-large-cc25_cp26k.jsonl\", claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported 1401/3000 records\n",
      "labels: Counter({'ok': 848, 'bad': 553})\n",
      "train split size: 1201, labels Counter({'ok': 727, 'bad': 474})\n",
      "dev split size: 100, labels Counter({'ok': 61, 'bad': 39})\n",
      "test split size: 100, labels Counter({'ok': 60, 'bad': 40})\n",
      "saved to /mnt/data/factcheck/wiki/cs/20230220/qacg/claim_quality_v3/PAV-ner-CNEC/mt5-large-cp59k/mbart-large-cc25_cp26k\n",
      "imported 1401/3000 records\n",
      "labels: Counter({'ok': 848, 'bad_grammar': 202, 'bad': 198, 'incomplete': 153})\n",
      "train split size: 1201, labels Counter({'ok': 726, 'bad_grammar': 174, 'bad': 170, 'incomplete': 131})\n",
      "dev split size: 100, labels Counter({'ok': 61, 'bad_grammar': 14, 'bad': 14, 'incomplete': 11})\n",
      "test split size: 100, labels Counter({'ok': 61, 'bad': 14, 'bad_grammar': 14, 'incomplete': 11})\n",
      "saved to /mnt/data/factcheck/wiki/cs/20230220/qacg/claim_quality_v3/PAV-ner-CNEC/mt5-large-cp59k/mbart-large-cc25_cp26k\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_classification_splits_from_annotated(src_jsonl, dst_dir, n_dev, n_test, all_classes=False):\n",
    "    src = read_jsonl(src_jsonl)\n",
    "    samples = []\n",
    "    label_cnt = Counter()\n",
    "    for s in src:\n",
    "        if len(s['label']) > 0:\n",
    "            # if len(s['label']) > 1:\n",
    "                # print(s)\n",
    "            assert len(s['label']) == 1, s\n",
    "            label = s[\"label\"][0]\n",
    "            if not all_classes:\n",
    "                if label != \"ok\": # move all negative samples to single \"bad\" class\n",
    "                    label = \"bad\"\n",
    "            label_cnt[label] += 1\n",
    "            samples.append({\"text\": s[\"text\"], \"label\": label})\n",
    "    print(f\"imported {len(samples)}/{len(src)} records\")\n",
    "    print(f\"labels: {label_cnt}\")\n",
    "    # the samples are already shuffled for annotation - see above, but I want stratified split, so we need to shuffle once more\n",
    "    dev_samples, train_samples = train_test_split(samples, train_size=n_dev, shuffle=True, stratify=[s[\"label\"] for s in samples], random_state=1234)\n",
    "    test_samples, train_samples = train_test_split(train_samples, train_size=n_test, shuffle=True, stratify=[s[\"label\"] for s in train_samples], random_state=1234)\n",
    "    print(f\"train split size: {len(train_samples)}, labels {Counter([s['label']for s in train_samples])}\")\n",
    "    print(f\"dev split size: {len(dev_samples)}, labels {Counter([s['label']for s in dev_samples])}\")\n",
    "    print(f\"test split size: {len(test_samples)}, labels {Counter([s['label']for s in test_samples])}\")\n",
    "    suffix = \"_all_classes\" if all_classes else \"\"\n",
    "    write_jsonl(Path(dst_dir, f\"train{suffix}.jsonl\"), train_samples, mkdir=True)\n",
    "    write_jsonl(Path(dst_dir, f\"dev{suffix}.jsonl\"), dev_samples, mkdir=True)\n",
    "    write_jsonl(Path(dst_dir, f\"test{suffix}.jsonl\"), test_samples, mkdir=True)\n",
    "    print(f\"saved to {dst_dir}\")\n",
    "\n",
    "\n",
    "create_classification_splits_from_annotated(\"/home/drchajan/all.jsonl\", CLAIM_QUALITY_ROOT, n_dev=100, n_test=100)\n",
    "create_classification_splits_from_annotated(\"/home/drchajan/all.jsonl\", CLAIM_QUALITY_ROOT, n_dev=100, n_test=100, all_classes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial counts: Counter({'ok': 727, 'bad': 474})\n",
      "balanced counts: Counter({'ok': 474, 'bad': 474})\n",
      "saved to: /mnt/data/factcheck/wiki/cs/20230220/qacg/claim_quality_v3/PAV-ner-CNEC/mt5-large-cp59k/mbart-large-cc25_cp26k/train_balanced.jsonl\n"
     ]
    }
   ],
   "source": [
    "def balance_split(split_dir, src_split, dst_split, seed=1234):\n",
    "    # balances by subsampling to the least present class\n",
    "    recs = read_jsonl(Path(split_dir, src_split))\n",
    "    label_cnts = Counter(r[\"label\"] for r in recs)\n",
    "    print(f\"initial counts: {label_cnts}\")\n",
    "    least_occurences = label_cnts.most_common()[-1][1]\n",
    "    ret = []\n",
    "    for r in recs:\n",
    "        l = r[\"label\"]\n",
    "        if label_cnts[l] == least_occurences:\n",
    "            ret.append(r)\n",
    "        else:\n",
    "            label_cnts[l] -= 1\n",
    "    print(f\"balanced counts: {label_cnts}\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(ret)\n",
    "    dst_path = Path(split_dir, dst_split)\n",
    "    write_jsonl(dst_path, ret)\n",
    "    print(f\"saved to: {dst_path}\")\n",
    "\n",
    "balance_split(CLAIM_QUALITY_ROOT, \"train.jsonl\", \"train_balanced.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vyhodnoť následující řadu tvrzení. Cílem je rozpoznat, která z nich by bylo možné a zajímavé ověřit. Udej klasifikaci: OK/ŠPATNÉ a pro třídu ŠPATNÉ i vysvětlení:\n",
      "Tvrzení: Doktor Proktor a velká loupež zlata vyšla v roce 2012.\n",
      "Třída: OK\n",
      "\n",
      "Tvrzení: František Sokol se narodil v roce 1968.\n",
      "Třída: OK\n",
      "\n",
      "Tvrzení: Světec je na obraze svatého Šebestiánu.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: celkově špatné\n",
      "\n",
      "Tvrzení: Vláda Ludvíka XIV. skončila v roce 74.\n",
      "Třída: OK\n",
      "\n",
      "Tvrzení: Karlo-Ferdinandova univerzita se nachází v Praze.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: špatná gramatika\n",
      "\n",
      "Tvrzení: Rodina bydlela na Židovském Městě pražském.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: příliš obecné nebo neúplné\n",
      "\n",
      "Tvrzení: Daniel Pearl se stal členem Wall Street Journal v roce 1990.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: celkově špatné\n",
      "\n",
      "Tvrzení: Trubadúr porazil koňskou oblast.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: celkově špatné\n",
      "\n",
      "Tvrzení: Bývalý pilot F1, který podpořil projekt, byl Adrian Newey.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: příliš obecné nebo neúplné\n",
      "\n",
      "Tvrzení: Aryna Sabalenková vyhrála 2. kolo.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: příliš obecné nebo neúplné\n",
      "\n",
      "Tvrzení: Eliza Radziwiłłovna se jmenovala Viléma I.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: špatná gramatika\n",
      "\n",
      "Tvrzení: Bílichovské údolí se nachází na potoku.\n",
      "Třída: ŠPATNÉ\n",
      "Vysvětlení: špatná gramatika\n",
      "\n",
      "Tvrzení: Továrna na spódium se nachází v Boskovicích.\n"
     ]
    }
   ],
   "source": [
    "def construct_prompt(split_dir, src_split, examples_per_cls=2, seed=1234):\n",
    "    recs = read_jsonl(Path(split_dir, src_split))\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(recs)\n",
    "    labels = set(r[\"label\"] for r in recs)\n",
    "    label_cnts = Counter()\n",
    "    for l in labels:\n",
    "        label_cnts[l] = examples_per_cls\n",
    "    sel = []\n",
    "    rest = []\n",
    "    for r in recs:\n",
    "        l = r[\"label\"]\n",
    "        if label_cnts[l] > 0:\n",
    "            sel.append(r)\n",
    "            label_cnts[l] -= 1\n",
    "        else:\n",
    "            rest.append(r)\n",
    "\n",
    "    prompt = \"Vyhodnoť následující řadu tvrzení. Cílem je rozpoznat, která z nich by bylo možné a zajímavé ověřit. Udej klasifikaci: OK/ŠPATNÉ a pro třídu ŠPATNÉ i vysvětlení:\"\n",
    "    for r in sel:\n",
    "        l = r['label']\n",
    "        prompt += f\"\\nTvrzení: {r['text']}\"\n",
    "        prompt += \"\\nTřída: \" + (\"OK\" if l == \"ok\" else \"ŠPATNÉ\")\n",
    "        if l == 'bad_grammar':\n",
    "            prompt += \"\\nVysvětlení: špatná gramatika\"\n",
    "        elif l == 'incomplete':\n",
    "            prompt += \"\\nVysvětlení: příliš obecné nebo neúplné\"\n",
    "        elif l == 'bad':\n",
    "            prompt += \"\\nVysvětlení: celkově špatné\"\n",
    "        prompt += \"\\n\"\n",
    "    prompt += f\"\\nTvrzení: {rest[3]['text']}\"\n",
    "    return prompt\n",
    "\n",
    "prompt = construct_prompt(CLAIM_QUALITY_ROOT, \"train_all_classes.jsonl\", examples_per_cls=3)\n",
    "print(prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env_plight_env",
   "language": "python",
   "name": "fc_env_plight_env"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
