{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter, OrderedDict\n",
    "import ujson\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import sys\n",
    "import textwrap\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Set, Union\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.fever import fever_detokenize, import_fever_corpus_from_sqlite\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "\n",
    "sys.path.append('Claim_Generation')\n",
    "from T5_QG import pipeline\n",
    "from distractor_generation import Distractor_Generation\n",
    "\n",
    "sys.path.append('Models')\n",
    "from arguments import ModelArguments, DataTrainingArguments\n",
    "from load import load_tokenizer_and_model, find_last_checkpoint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is full version aimed at generating SUPPORTED and REFUTED claims needed for evidence retrieval, keeping NEIs for later.\n",
    "- It is a simplified version of the `claim_generation_paragraphs_wiki.ipynb`.\n",
    "- Aimed to generate data for post LREV EnFEVER models (e.g., ColBERT v2) for CsFEVER corpus.\n",
    "- Fixed input and output formats for those we use in AIC.\n",
    "\n",
    "**Notes**\n",
    "- Currently ignoring multi-hops - single evidence documents are used only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEVER_ROOT = Path(\"/mnt/data/factcheck/fever/data_full_nli-filtered-cs\")\n",
    "FEVER_DATA = Path(FEVER_ROOT, \"fever-data/F1_titles_anserininew_threshold\")\n",
    "FEVER_CORPUS_SQLITE = Path(FEVER_ROOT, \"fever/cs_wiki_revid_db_sqlite.db\")\n",
    "QACG_ROOT = Path(FEVER_ROOT, \"qacg\")\n",
    "QACG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TODO move to utils\n",
    "def batch_apply(func, data, batch_size) -> List:\n",
    "    n = len(data)\n",
    "    first = 0\n",
    "    res = []\n",
    "    while first < n:\n",
    "        last = min(first + batch_size, n)\n",
    "        res += func(data[first:last])\n",
    "        first = last\n",
    "    assert n == len(res)\n",
    "    return res\n",
    "\n",
    "def nfc(s: str):\n",
    "    return unicodedata.normalize(\"NFC\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/wiki-pages/**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1476/1476 [00:11<00:00, 124.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# all: 825078\n",
      "# without duplicate texts: 501199\n",
      "# without short texts: 501158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501158/501158 [00:00<00:00, 667662.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# without text removed based on RE: 501142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501142/501142 [00:05<00:00, 84824.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fixed HTML: 3992, remaining: 0, errors: 0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import html\n",
    "import re\n",
    "from lxml.html.clean import Cleaner\n",
    "\n",
    "# TODO: this is the same code as in drchajan:/download_wiki.ipynb; move it to aic_nlp_utils?\n",
    "# this version is slightly modified (glob string & textcol parameter)\n",
    "\n",
    "def _filter_and_fix_wiki_extract(root_dir, min_length, textcol=\"text\", remove_re=None):\n",
    "    # min_length - shorter pages (text) are removed\n",
    "    # remove_re - matching pages (text) are removed\n",
    "    records = []\n",
    "    glob_str = f\"{root_dir}/**\"\n",
    "    print(glob_str)\n",
    "    source_jsonls = sorted(glob.glob(glob_str))\n",
    "    for source_json in tqdm(source_jsonls):\n",
    "        for r in read_jsonl(source_json):\n",
    "            r[\"original_id\"] = int(r[\"id\"])\n",
    "            r[\"id\"] = r[\"title\"].strip()\n",
    "            r[\"text\"] = html.unescape(r[textcol].strip())\n",
    "            if textcol != \"text\":\n",
    "                del r[textcol]\n",
    "            r[\"id\"] = nfc(r[\"id\"])\n",
    "            r[\"title\"] = nfc(r[\"title\"])\n",
    "            r[\"text\"] = nfc(r[\"text\"])\n",
    "            records.append(r)\n",
    "    print(f\"# all: {len(records)}\")\n",
    "    cntr = Counter((r[\"text\"] for r in records))\n",
    "    records = [r for r in records if cntr[r[\"text\"]] == 1]\n",
    "    print(f\"# without duplicate texts: {len(records)}\")\n",
    "    records = [r for r in records if len(r[\"text\"]) >= min_length]\n",
    "    print(f\"# without short texts: {len(records)}\")\n",
    "    if remove_re:\n",
    "        pattern = re.compile(remove_re)\n",
    "        records = [r for r in tqdm(records) if not re.match(pattern, r[\"text\"])]\n",
    "        print(f\"# without text removed based on RE: {len(records)}\")\n",
    "    return records\n",
    "\n",
    "def _fix_html(records, kill_tags, remove_tags, remove_text):\n",
    "    cleaner = Cleaner(page_structure=False, links=False, kill_tags=kill_tags, remove_tags=remove_tags)\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "    n_html = 0\n",
    "    n_unfixed = 0\n",
    "    n_err = 0\n",
    "    for r in tqdm(records):\n",
    "        for rt in remove_text:\n",
    "            r[\"text\"] = r[\"text\"].replace(rt, \"\")\n",
    "        if \"<\" in r[\"text\"]: # simple rough tag detection\n",
    "            n_html += 1\n",
    "            try:\n",
    "                r[\"text\"] = cleaner.clean_html(r[\"text\"])\n",
    "            except:\n",
    "                n_err += 1\n",
    "            r[\"text\"] = TAG_RE.sub('', r[\"text\"]).strip()\n",
    "            if \"<\" in r[\"text\"]:\n",
    "                n_unfixed += 1\n",
    "    print(f\"# fixed HTML: {n_html}, remaining: {n_unfixed}, errors: {n_err}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def filter_and_fix_wiki_extract_for_lang(root_dir, output_jsonl, lang, textcol=\"text\"):\n",
    "    assert lang in [\"cs\", \"en\", \"pl\", \"sk\"]\n",
    "    remove_text = []\n",
    "    if lang == \"cs\":\n",
    "        min_length = 10\n",
    "        remove_re = r\"\\#PŘESMĚRUJ[^\\n]*|\\#REDIRECT[^\\n]*|\\#redirect[^\\n]*\"\n",
    "        kill_tags = [\"id\", \"minor\", \"ns\", \"parentid\", \"timestamp\", \"contributor\", \"comment\", \"model\", \"format\", \"templatestyles\"]\n",
    "        remove_tags = [\"revision\"]\n",
    "        remove_text = [\"Externí odkazy.\", \"Odkazy.\", \"Reference.\", \"Literatura.\", \"Související články.\", '\"V tomto článku byl použit textu z článku .\"', ': | | | | |', ': | | | |', ': | | |', ': | |', ': |', '• \\xa0 \\xa0 • • • •', '\"V tomto článku byly použity textů z článků']\n",
    "    elif lang == \"en\":\n",
    "        min_length = 10\n",
    "        remove_re=r\"[^\\n]* refers? to:?$|[^\\n]* stands? for:?$|[^\\n]*\\n.* refers? to:?$|[^\\n]*refers? to:\\n[^\\n]*|[^\\n]*the following:?$|[^\\n]* mean:?$|[^\\n]* be:$|[^\\n]* is:$|\\#REDIRECT[^\\n]*\"\n",
    "        kill_tags = [\"ref\", \"onlyinclude\"]\n",
    "        remove_tags = []\n",
    "    elif lang == \"pl\":\n",
    "        min_length = 30\n",
    "        remove_re=r\"[^\\n]* to:$|\\#PATRZ[^\\n]*|[^\\n]* może oznaczać:$\"\n",
    "        kill_tags = []\n",
    "        remove_tags  = [\"poem\"]\n",
    "        remove_text = [\"\\right]</math>\"]\n",
    "    elif lang == \"sk\":\n",
    "        min_length = 10\n",
    "        remove_re=r\"[^\\n]* je:?$|[^\\n]* môže byť:?$\"\n",
    "        kill_tags = [\"indicator\"]\n",
    "        remove_tags  = []\n",
    "        remove_text = ['je zatiaľ „“. Pomôž Wikipédii tým, že ho [ doplníš a rozšíriš.]\"']\n",
    "\n",
    "    records = _filter_and_fix_wiki_extract(root_dir, min_length, remove_re=remove_re, textcol=textcol)\n",
    "    records = _fix_html(records, kill_tags, remove_tags, remove_text)\n",
    "    write_jsonl(output_jsonl, records)\n",
    "    return records\n",
    "\n",
    "EXTRACTED_ROOT = \"/mnt/data/factcheck/fever/data_full_nli-filtered-cs/\"\n",
    "corpus = filter_and_fix_wiki_extract_for_lang(\n",
    "    Path(EXTRACTED_ROOT, \"wiki-pages\"),\n",
    "    Path(EXTRACTED_ROOT, \"fever\", \"wiki_extract_filtered_and_fixed_drchajan.jsonl\"), \"cs\", textcol=\"contents\")\n",
    "corpus_id2idx = {r[\"id\"]: i for i, r in enumerate(corpus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'České středohoří',\n",
       " 'revid': '503435',\n",
       " 'url': 'https://cs.wikipedia.org/wiki?curid=4612',\n",
       " 'title': 'České středohoří',\n",
       " 'original_id': 4612,\n",
       " 'text': 'České středohoří () je geomorfologický celek o rozloze 1265 km². Z hlediska horopisného patří do Podkrušnohorské oblasti, která je součástí Krušnohorské subprovincie. Na 84 % území Českého středohoří zaujímá Chráněná krajinná oblast České středohoří (CHKO České středohoří) o výměře 1063,17 km². Nejvyšším vrcholem je Milešovka (837 m). Nejnižším bodem je hladina Labe v Děčíně (121,9 m). Maximální výškový rozdíl tedy činí 715,1 m. Geomorfologické členění. Hluboké údolí Labe rozděluje České středohoří na dva geomorfologické podcelky: Verneřické středohoří (IIIB-5A) na pravém břehu Labe a Milešovské středohoří (IIIB-5B) na levém břehu Labe. Tyto podcelky se dále člení do celkem osmi okrsků: Geologie. Rozlohou 1266 km², délkou přes 70 km a šířkou až 25 km patří České středohoří k menším orografickým celkům. Přesto je však nejmohutnějším projevem sopečné činnosti v Česku. České středohoří totiž vzniklo sopečnou činností. Podél řeky Ohře existoval Ohárecký rift, kterým se žhavé magma dostalo na povrch. V oblasti převažují čedičové horniny (73,6%), zbytek tvoří trachytické a v malé míře andezitické horniny. Územím prochází Litoměřický hlubinný zlom, který z geologického hlediska tvoří hranici mezi krušnohorskou a středočeskou oblastí. Pod povrchem se hromadilo magma v žilách a tvořily se tzv. lakolity, což byly podpovrchové balvany z utuhlého magmatu. V mladších třetihorách, v miocénu (asi před 23 miliony let) se začaly vyzdvihovat z pískovcového podloží sopečné kužely. V pliocénu (před 4,8 miliony let) vulkanity místy prorážely Českou křídovou pánev (Trosky, Kunětická hora). Vodní toky obnažily ztuhlé podpovrchové magma a prohlubovaly údolí, což dalo Českému středohoří majestátní krajinný ráz. Jedním takovým údolím je Porta Bohemica, kterou vymodelovala řeka Labe. Z výlevných hornin tu převažují čediče a znělce, z usazenin pískovce a opuky. Vrcholy. K významným vrchům kromě nejvyšší hory Milešovky (837 m) patří Hradišťany (753 m), Kletečná (706 m) a Lovoš (570 m). Ve východní části, oddělené Labem, jsou kopce poněkud nižší – nejvyšší z nich je Sedlo (727 m). Vodní toky. Osu středohoří tvoří úrodné údolí Labe. Dalším větším tokem je Ploučnice. Centrální části obou podcelků jsou odvodňovány drobnými toky směřujícími k Labi, Ploučnici a Bílině. Na labských přítocích v okolí Ústí nad Labem jsou zpětnou erozí vytvořeny vysoké vodopády (Moravanský, Vaňovský, ve Vlčí rokli, u Budova, v údolí Peklo u Týniště). Nejvyšší z nich má 12 m. Lesy. Lesnatost je malá a dosahuje 28,4 %. Z druhů je nejvíce zastoupen smrk (32,8 %) a dále se vyskytuje dub, buk, habr, bříza, jasan, lípa a javor. Drobná fauna a flóra. České středohoří má, díky své přirozené uzavřené poloze ze severu a otevřené z jihu, příznivé podmínky pro výskyt náročnějších druhů rostlin a živočichů, běžně žijících v teplých krajích. Roste zde například lilie zlatohlavá. Lidová architektura. Pro České středohoří je specifická venkovská sídelní struktura, daná hustou sítí malých obcí, osad a drobných sídel. Početné jsou památky lidové architektury, prolíná se tu několik regionálních typů lidových staveb (objekty zděné, roubené, hrázděné). O bohaté historii svědčí i několik předslovanských a slovanských hradišť, řada středověkých hradů na vrcholech kopců, tvrzí, zámků a dalších šlechtických sídel.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[2003]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following extracts corpus pages used as evidence in annotated CsFEVER data. We can use any page, but this will give us better comparison of what QACG generates when compared to FEVER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/fever-data/F1_titles_anserininew_threshold/train_fb_cs_nli_split_F1_titles_anserininew.jsonl\n",
      "missing pages: 53/5084\n",
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/fever-data/F1_titles_anserininew_threshold/paper_dev_fb_cs_nli_split_F1_titles_anserininew.jsonl\n",
      "missing pages: 3/553\n",
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/fever-data/F1_titles_anserininew_threshold/paper_test_fb_cs_nli_split_F1_titles_anserininew.jsonl\n",
      "missing pages: 0/584\n"
     ]
    }
   ],
   "source": [
    "# note, this differs from EnFEVER!\n",
    "# Tomas Mlynar uses different format...\n",
    "def extract_fever_evidence_pages(split_jsonls: List, corpus_id2idx: Dict, corpus):\n",
    "    fever_pages = set()\n",
    "    corpus_records = []\n",
    "    not_found = 0\n",
    "    for jsonl in split_jsonls:\n",
    "        print(jsonl)\n",
    "        split = read_jsonl(jsonl)\n",
    "        for rec in split:\n",
    "            if rec[\"verifiable\"] == \"VERIFIABLE\":\n",
    "                for ev in rec[\"evidence_cs\"].keys():\n",
    "                    ev = nfc(ev)\n",
    "                    if ev in corpus_id2idx:\n",
    "                        if ev not in fever_pages:\n",
    "                            corpus_records.append(corpus[corpus_id2idx[ev]])\n",
    "                        fever_pages.add(ev)\n",
    "                    else:\n",
    "                        not_found += 1\n",
    "    print(f\"missing pages: {not_found}/{not_found+len(fever_pages)}\")\n",
    "    return fever_pages, corpus_records\n",
    "\n",
    "\n",
    "\n",
    "fever_pages_trn, corpus_recs_trn = extract_fever_evidence_pages([Path(FEVER_DATA, \"train_fb_cs_nli_split_F1_titles_anserininew.jsonl\")], corpus_id2idx, corpus)\n",
    "fever_pages_dev, corpus_recs_dev = extract_fever_evidence_pages([Path(FEVER_DATA, \"paper_dev_fb_cs_nli_split_F1_titles_anserininew.jsonl\")], corpus_id2idx, corpus)\n",
    "fever_pages_tst, corpus_recs_tst = extract_fever_evidence_pages([Path(FEVER_DATA, \"paper_test_fb_cs_nli_split_F1_titles_anserininew.jsonl\")], corpus_id2idx, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Jeff Bridges',\n",
       " 'revid': '546074',\n",
       " 'url': 'https://cs.wikipedia.org/wiki?curid=481829',\n",
       " 'title': 'Jeff Bridges',\n",
       " 'original_id': 481829,\n",
       " 'text': 'Jeffrey Leon „Jeff“ Bridges (* 4. prosince 1949 v Los Angeles v Kalifornii, USA) je americký herec a hudebník. Mezi jeho nejvýznamnější filmy patři \"Poslední představení\" (v originále \"The Last Picture Show\"), \"Tron\", \"Starman\", \"Báječní Bakerovi hoši (The Fabulous Baker Boys)\", \"Král rybář (The Fisher King)\", \"Beze strachu (Fearless)\", \"Big Lebowski\", \"Kandidáti (The Contender)\" a \"Iron Man\". Českým divákům je znám svojí rolí Jacka Prescotta v americkém dobrodružném filmu režiséra Johna Guillermina King Kong z roku 1976. Osobní život. Narodil se v Los Angeles v Kalifornii manželům Lloydovi a Dorothy Bridgesovým. Má staršího bratra Beaua a mladší sestru Lucindu. Jeho druhý bratr Garret zemřel v roce 1948 na syndrom náhlého úmrtí kojenců. Se svým bratrem Beauem, který se o něj staral, když byl jeho otec zaneprázdněn prací, měl velice blízký vztah. Spolu se svými sourozenci vyrůstal ve čtvrti Holmby Hills v Los Angeles. Je strýcem Jordana Bridgese. V roce 1977 se oženil se Susan Gestonovou, se kterou se setkal během natáčení filmu \"Rancho Deluxe\" na ranči, kde Susan pracovala jako služebná. Mají spolu tři dcery: Isabellu (narozená v roce 1981), Jessicu Lily (1983) a Hayley Roselouise (1985). Je také kuřákem marihuany; v rozhovoru přiznal, že v průběhu natáčení filmu Big Lebowski sice nekouřil, ale „návyku se natrvalo nezbavil“. Obdržel humanitární cenu Action Against Hunger. Filmová kariéra. Jako teenager se spolu se svým bratrem Beauem objevoval v pořadu jejich otce \"The Lloyd Bridges Show\". Svoji první velkou filmovou roli získal ve snímku \"Poslední představení (The Last Picture Show)\" z roku 1971, za kterou získal nominaci na Oscara za nejlepší mužský herecký výkon ve vedlejší roli. Na tutéž cenu byl opět nominován za svůj výkon po boku Clinta Eastwooda ve filmu \"Thunderbolt a Lightfoot\" (1974). Mezi jeho známější role patří také postava Kevina Flynna v kultovním sci-fi filmu \"Tron\" nebo mimozemšťan ve snímku \"Starman\", za kterého byl nominován na Oscara. Hrál i Jeffa „Dudea“ Lebowského v kultovním snímku \"Big Lebowski\". Jeho výkon ve filmu \"Fearless\" je některými kritiky označován za jeden z jeho nejlepších. Jeden z nich ho nazval mistrovským dílem; Pauline Kaelová napsala, že „je možná ten nejpřirozenější a nejméně rozpačitý filmový herec, který kdy žil.“ V roce 2000 byl za svou roli v \"Kandidátech\" (v originále \"The Contender\") nominován na svého čtvrtého Oscara. Hrál také ve filmu \"Krajina přílivu (Tideland, 2005)\" režiséra Terryho Gilliama, což byla už druhá role, kterou pod jeho vedením ztvárnil (tou první byl \"Král rybář (The Fisher King)\" v roce 1991). Ve snímku společnosti Marvel \"Iron Man\" z roku 2008 hraje postavu Obadiaha Stane. V roce 2010 si si zopakoval roli Kevina Flynna ve snímku \"\". V roce 2010 vyhrál Zlatý glóbus v kategorii Nejlepší herec v dramatu a Oscara za Nejlepší mužský herecký výkon v hlavní roli za roli ve filmu \"Crazy Heart\".'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_recs_trn[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER\n",
    "Not one of the CZECH NER models extracts ordinals or cardinals, i.e., numbers beyond dates. This might be a problem. We need a better one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertTokenizerFast\n",
    "from transformers import pipeline\n",
    "from ufal.nametag import Ner, Forms, TokenRanges, NamedEntities\n",
    "\n",
    "\n",
    "def load_czert_ner_pipeline(model_name):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    tokenizer = BertTokenizerFast(Path(model_name, \"vocab.txt\"), strip_accents=False, do_lower_case=False, truncate=True, model_max_length=512)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\", device=device)\n",
    "    def ner_pipeline_pairs(text):\n",
    "        ner_dicts = ner_pipeline(text)\n",
    "        # ner_pairs = [(e[\"word\"], e[\"entity_group\"]) for e in ner_dicts]\n",
    "        ner_pairs = [(text[e[\"start\"]:e[\"end\"]], e[\"entity_group\"]) for e in ner_dicts]\n",
    "        return ner_pairs\n",
    "    return ner_pipeline_pairs\n",
    "\n",
    "\n",
    "class UFALNERExtractor:\n",
    "    def __init__(self, model):\n",
    "        self.ner = Ner.load(model)\n",
    "        self.forms = Forms()\n",
    "        self.tokens = TokenRanges()\n",
    "        self.entities = NamedEntities()\n",
    "        self.tokenizer = self.ner.newTokenizer()\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        self.tokenizer.setText(text)\n",
    "        ners = []\n",
    "        nertypes = []\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            self.ner.recognize(self.forms, self.entities)\n",
    "            \n",
    "            entities = sorted(self.entities, key=lambda entity: (entity.start, -entity.length))\n",
    "            \n",
    "            prev_end = -1\n",
    "            for entity in entities:\n",
    "                if (entity.start + entity.length) <= prev_end: # take only the highest level entities\n",
    "                    continue\n",
    "                ners.append(\" \".join(self.forms[entity.start:entity.start+entity.length]))\n",
    "                nertypes.append(entity.type)\n",
    "                prev_end = entity.start + entity.length\n",
    "        ner_pairs = [(ner, nertype) for ner, nertype in zip(ners, nertypes)]\n",
    "        return ner_pairs\n",
    "\n",
    "\n",
    "def extract_ners(corpus_recs, ner_json, model_name):\n",
    "    # for each text gives a triplet (ner, ner_type, ner-ner_type count in text)\n",
    "    # the triplets are sorted by decreasing count\n",
    "\n",
    "    if model_name == \"ufal.nametag\":\n",
    "        ner_pipeline = UFALNERExtractor(\"/mnt/data/factcheck/ufal/ner/czech-cnec2.0-140304-no_numbers.ner\")\n",
    "    else:\n",
    "        ner_pipeline = load_czert_ner_pipeline(model_name)\n",
    "    entity_dict = OrderedDict()\n",
    "    for l in tqdm(corpus_recs):\n",
    "        text = l[\"text\"]\n",
    "        ner_pairs = ner_pipeline(text)\n",
    "        ner_cnts = Counter(ner_pairs) # their \n",
    "        ners_unique_with_counts =  [(p[0], p[1], ner_cnts[(p[0], p[1])]) for p in set(ner_pairs)]\n",
    "        ners_unique_with_counts = sorted(ners_unique_with_counts, key=lambda n: -n[2])\n",
    "        entity_dict[l[\"id\"]] = ners_unique_with_counts\n",
    "    write_json(ner_json, entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:07<00:00, 68.96it/s] \n",
      "100%|██████████| 584/584 [00:09<00:00, 61.10it/s] \n",
      "100%|██████████| 5031/5031 [01:05<00:00, 76.71it/s] \n"
     ]
    }
   ],
   "source": [
    "model_short=\"ufal.nametag\"\n",
    "model_name=\"ufal.nametag\"\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"ner\", f\"dev_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"ner\", f\"test_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"ner\", f\"train_ners-{model_short}.json\"), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/550 [00:00<00:27, 19.86it/s]/home/drchajan/devel/python/FC/fc_env_plight_env/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 550/550 [00:11<00:00, 46.11it/s]\n",
      "100%|██████████| 584/584 [00:12<00:00, 46.48it/s]\n",
      "100%|██████████| 5031/5031 [01:43<00:00, 48.59it/s]\n"
     ]
    }
   ],
   "source": [
    "model_short=\"CZERT-B-ner-CNEC\"\n",
    "model_name=f\"/mnt/data/factcheck/models/czert/{model_short}\"\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"ner\", f\"dev_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"ner\", f\"test_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"ner\", f\"train_ners-{model_short}.json\"), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [00:11<00:00, 46.49it/s]\n",
      "100%|██████████| 584/584 [00:13<00:00, 43.53it/s]\n",
      "100%|██████████| 5031/5031 [01:49<00:00, 46.10it/s]\n"
     ]
    }
   ],
   "source": [
    "model_short=\"PAV-ner-CNEC\"\n",
    "model_name=f\"/mnt/data/factcheck/models/czert/{model_short}\"\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"ner\", f\"dev_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"ner\", f\"test_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"ner\", f\"train_ners-{model_short}.json\"), model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not merging outputs of multiple NER methods anymore -- keeping for future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ners(fins, fout):\n",
    "    iners = [read_json(fin) for fin in fins]\n",
    "    page2ners = defaultdict(dict)\n",
    "    for iner in iners:\n",
    "        for page, ners in iner.items():\n",
    "            for ner in ners:\n",
    "                word, type_, cnt = ner\n",
    "                if word in page2ners[page]:\n",
    "                    if page2ners[page][word][1] >= cnt:\n",
    "                        continue\n",
    "                page2ners[page][word] = [type_, cnt]\n",
    "    page2nerlsts = {page: sorted([[word] + params for word, params in ners.items()], key=lambda n: -n[2]) for page, ners in page2ners.items()}\n",
    "    write_json(fout, page2nerlsts)\n",
    "\n",
    "merge_ners(\n",
    "    [Path(QACG_ROOT, \"ner\", \"dev_ners-CZERT-B-ner-CNEC.json\"),\n",
    "     Path(QACG_ROOT, \"ner\", \"dev_ners-PAV-ner-CNEC.json\")], \n",
    "    Path(QACG_ROOT, \"ner\", \"dev_ners.json\"))\n",
    "\n",
    "merge_ners(\n",
    "    [Path(QACG_ROOT, \"ner\", \"test_ners-CZERT-B-ner-CNEC.json\"),\n",
    "     Path(QACG_ROOT, \"ner\", \"test_ners-PAV-ner-CNEC.json\")], \n",
    "    Path(QACG_ROOT, \"ner\", \"test_ners.json\"))\n",
    "\n",
    "merge_ners(\n",
    "    [Path(QACG_ROOT, \"ner\", \"train_ners-CZERT-B-ner-CNEC.json\"),\n",
    "     Path(QACG_ROOT, \"ner\", \"train_ners-PAV-ner-CNEC.json\")], \n",
    "    Path(QACG_ROOT, \"ner\", \"train_ners.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation (QG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchQuestionGenerator:\n",
    "    def __init__(self, tokenizer, model, highlight=False, highlight_tag=\"<hl>\", max_source_length=1024, padding=False, device=\"cuda\", debug=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(device)\n",
    "        self.highlight = highlight\n",
    "        self.highlight_tag = highlight_tag\n",
    "        self.max_source_length = max_source_length\n",
    "        self.padding = padding\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "\n",
    "    def generate(self, contexts, answers, batch_size=32):\n",
    "        def highlight_fun(answer, context):\n",
    "            offset = context.index(answer)\n",
    "            return f\"{context[:offset]}<hl>{answer}<hl>{context[offset + len(answer):]}\"\n",
    "\n",
    "        n = len(contexts)\n",
    "        assert n == len(answers), (n, len(answers))\n",
    "        offset = 0\n",
    "        failures = 0\n",
    "        predictions = []\n",
    "        while offset < n:\n",
    "            last = min(offset + batch_size, n)\n",
    "            if self.highlight:\n",
    "                inputs = []\n",
    "                for context, answer in zip(contexts[offset:last], answers[offset:last]):\n",
    "                    # if answer in context:\n",
    "                    inputs.append(highlight_fun(answer, context) )\n",
    "                    # else:\n",
    "                        # failures += 1\n",
    "            else:\n",
    "                inputs = [answer + \"</s>\" + context for context, answer in zip(contexts[offset:last], answers[offset:last])]\n",
    "            model_inputs = self.tokenizer(inputs, max_length=self.max_source_length, padding=self.padding, truncation=True, return_tensors=\"pt\")\n",
    "            model_inputs = {k: v.to(self.device) for k, v in model_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                Y = self.model.generate(**model_inputs, max_new_tokens=768)\n",
    "                batch_predictions = self.tokenizer.batch_decode(\n",
    "                    Y, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            predictions += batch_predictions\n",
    "            offset += batch_size\n",
    "\n",
    "        assert n == len(predictions)\n",
    "        if self.debug:\n",
    "            for input, pred in zip(inputs, predictions):\n",
    "                print(textwrap.fill(input))\n",
    "                print()\n",
    "                print(pred)\n",
    "                print(\"----------------------------\")\n",
    "        # print(f\"#failures: {failures}, #predictions: {len(predictions)}/{n}\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight = False\n",
    "# def batch_evaluator():\n",
    "# model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/facebook/mbart-large-cc25_cs_CZ/checkpoint-10000\")\n",
    "# model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/google/mt5-base_cs_CZ/checkpoint-40000\")\n",
    "# model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/google/mt5-large_cs_CZ/checkpoint-59000\")\n",
    "\n",
    "highlight = True\n",
    "model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/google/mt5-large_cs_CZ_hl/checkpoint-48000\")\n",
    "\n",
    "tokenizer, model, data_collator = load_tokenizer_and_model(model_args, lang=\"cs_CZ\", fp16=True)\n",
    "\n",
    "batch_question_generator = BatchQuestionGenerator(tokenizer, model, highlight=True, padding=True, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qas(corpus_recs, ner_json, qas_json, generator):\n",
    "    # QG NLP object\n",
    "\n",
    "    # print('Loading QG module >>>>>>>>')\n",
    "    # print('QG module loaded.')\n",
    "\n",
    "    ners = read_json(ner_json)\n",
    "\n",
    "    qas = OrderedDict()\n",
    "    invalid_sample = 0\n",
    "    for l in tqdm(corpus_recs):\n",
    "        id_ = str(l['id'])\n",
    "        if id_ not in ners: # no NERs in this text\n",
    "            continue\n",
    "        entities = ners[id_]\n",
    "\n",
    "        # create a batch\n",
    "        contexts, answers = [], []\n",
    "        for ent_text, ent_type, ent_cnt in entities:\n",
    "            contexts.append(l['text'])\n",
    "            answers.append(ent_text)\n",
    "\n",
    "\n",
    "        # question generation\n",
    "        if len(contexts) > 0 and len(contexts) == len(answers):\n",
    "            questions = []\n",
    "            # try:\n",
    "            questions = generator.generate(contexts, answers)\n",
    "            # except:\n",
    "                # invalid_sample += 1\n",
    "\n",
    "            if len(questions) == 0:\n",
    "                continue\n",
    "            \n",
    "            assert len(questions) == len(contexts)\n",
    "            # save results\n",
    "            result_for_sample = {}\n",
    "            for entity, question, answer, context in zip(entities, questions, answers, contexts):\n",
    "                ent_text, ent_type, _ = entity\n",
    "                result_for_sample[f'{ent_text}:::{ent_type}'] = [question, answer]\n",
    "\n",
    "            qas[str(l['id'])] = result_for_sample\n",
    "        else:\n",
    "            invalid_sample += 1\n",
    "\n",
    "    # print(f'#invalid samples: {invalid_sample}')\n",
    "    Path(qas_json).parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_json(qas_json, qas)\n",
    "\n",
    "\n",
    "# generate_qas(corpus_recs_dev[0:1], Path(QACG_ROOT, \"ner\", \"dev_ners.json\"), Path(QACG_ROOT, \"qa\", \"dev_qas.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_dev, Path(QACG_ROOT, \"ner\", \"dev_ners-PAV-ner-CNEC.json\"), Path(QACG_ROOT, \"qa\", \"dev_qas-PAV-ner-CNEC_cp10000.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_dev, Path(QACG_ROOT, \"ner\", \"dev_ners-PAV-ner-CNEC.json\"), Path(QACG_ROOT, \"qa\",\"dev_qas-PAV-ner-CNEC_mt5-base-cp40000.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_tst, Path(QACG_ROOT, \"ner\", \"test_ners.json\"), Path(QACG_ROOT, \"qa\", \"test_qas.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_trn, Path(QACG_ROOT, \"ner\", \"train_ners.json\"), Path(QACG_ROOT, \"qa\", \"train_qas.json\"), batch_question_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550/550 [22:13<00:00,  2.43s/it] \n",
      "100%|██████████| 584/584 [24:16<00:00,  2.49s/it]\n",
      "100%|██████████| 5031/5031 [3:19:49<00:00,  2.38s/it]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/fever/data_full_nli-filtered-cs/qacg')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confs = [\n",
    "    (corpus_recs_dev, \"dev\"),\n",
    "    (corpus_recs_tst, \"test\"),\n",
    "    (corpus_recs_trn, \"train\"),\n",
    "]\n",
    "\n",
    "for corpus_recs, name in confs:\n",
    "    generate_qas(corpus_recs, Path(QACG_ROOT, \"ner\", f\"{name}_ners-PAV-ner-CNEC.json\"), Path(QACG_ROOT, \"qa\", f\"{name}_qas-PAV-ner-CNEC_mt5-large-cp59000.json\"), batch_question_generator)\n",
    "\n",
    "QACG_ROOT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim Generator (QA2D + Replacement Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for testing\n",
    "# QA2D_model_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qa2d/facebook/mbart-large-cc25_cs_CZ/checkpoint-26000\"\n",
    "# sense_to_vec_path=\"dependencies/s2v_old\"\n",
    "\n",
    "# model_args = ModelArguments(model_name_or_path=QA2D_model_path)\n",
    "# tokenizer, model, data_collator = load_tokenizer_and_model(model_args, lang=\"cs_CZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameDocumentNERReplacementGenerator:\n",
    "    def __init__(self, seed=1234):\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "    def get_options(self, answer, entity, passage_entities, **kwargs):\n",
    "        ent_name, ent_type = entity.split(\":::\")\n",
    "        selected_entity_names = set()\n",
    "        for passage_entity in passage_entities:\n",
    "            pent_name, pent_type = passage_entity.split(\":::\")\n",
    "            if pent_type == ent_type and pent_name != ent_name:\n",
    "                selected_entity_names.add(pent_name)\n",
    "        if len(selected_entity_names) == 0:\n",
    "            return None\n",
    "        selected_entity_names = list(selected_entity_names)\n",
    "        selected_entity_name = self.rng.choice(selected_entity_names)\n",
    "        selected_entity = (selected_entity_name, ent_type)\n",
    "        # print(f\"{entity} -> {selected_entity}\")\n",
    "        return selected_entity\n",
    "    \n",
    "# replacement_generator = Distractor_Generation(sense2vec_path=sense_to_vec_path, T=0.7) # original EN replacement generator\n",
    "replacement_generator = SameDocumentNERReplacementGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimGenerator:\n",
    "    def __init__(self, replacement_generator, corpus_recs, ner_json, qas_json, QA2D_model_path, device=\"cuda\"):\n",
    "        # QA2D model object\n",
    "        print('Loading QA2D module >>>>>>>>')\n",
    "        model_args = ModelArguments(model_name_or_path=QA2D_model_path)\n",
    "        self.tokenizer, self.model, data_collator = load_tokenizer_and_model(model_args, lang=\"cs_CZ\")\n",
    "        print(f'Running on device: {device}')\n",
    "        # self.model, self.tokenizer = model, tokenizer # TODO REMOVE\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.replacement_generator = replacement_generator\n",
    "\n",
    "        self.corpus_recs = corpus_recs\n",
    "        self.ners = read_json(ner_json)\n",
    "        self.qas = read_json(qas_json)\n",
    "\n",
    "    def predict(self, inputs, max_source_length=1024, batch_size=16):\n",
    "        def pred_func(input_texts: List[str]) -> List[str]:\n",
    "            with torch.no_grad():\n",
    "                X = self.tokenizer(input_texts, max_length=max_source_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                X = {k: X[k].to(self.device) for k in X.keys()}\n",
    "                Y = self.model.generate(**X, max_new_tokens=768)\n",
    "                output_texts = self.tokenizer.batch_decode(\n",
    "                    Y, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            return output_texts\n",
    "            \n",
    "        predictions = batch_apply(pred_func, inputs, batch_size=batch_size)\n",
    "        return predictions\n",
    "\n",
    "    def _load_passage_entities(self, id_):\n",
    "        passage_entities = []\n",
    "        for ent_text, ent_type, _ in self.ners[id_]:\n",
    "            passage_entities.append(f'{ent_text}:::{ent_type}') # group by entity name and type as in the QAS file\n",
    "        return passage_entities\n",
    "    \n",
    "    def _load_precomputed_qas_for_entities(self, id_, passage_entities):\n",
    "        if id_ not in self.qas:\n",
    "            print(f\"missing id: {id_}\")\n",
    "            return None\n",
    "        QA_for_sample = self.qas[id_]\n",
    "        QA_pairs = []\n",
    "        for entity in passage_entities:\n",
    "            if entity in QA_for_sample:\n",
    "                ent_text, ent_type = entity.split(':::')\n",
    "                question, answer = QA_for_sample[entity]\n",
    "                QA_pairs.append({'question': question, 'answer': answer, 'answer_type': ent_type})\n",
    "            else:\n",
    "                print(f\"missing entity: {entity} for id: {id_}\")\n",
    "                return None\n",
    "        if len(QA_pairs) == 0:\n",
    "            print(f\"zero length pairs for id: {id_}\")\n",
    "            return None\n",
    "        return QA_pairs \n",
    "        \n",
    "\n",
    "    def generate_supported_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "\n",
    "        # Step 2: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 3: QA2D\n",
    "        # to_predict = [qa['question'] + ' [SEP] ' + qa['answer'] for qa in QA_pairs] # original model\n",
    "        to_predict = [qa['answer'] + '</s>' + qa['question'] for qa in QA_pairs]\n",
    "        results = []\n",
    "        # try:\n",
    "        results = self.predict(to_predict)\n",
    "        # except:\n",
    "            # return None\n",
    "        if len(results) == 0:\n",
    "            print(f\"zero length results for id: {id_}\")\n",
    "            return None\n",
    "\n",
    "        assert len(results) == len(QA_pairs)\n",
    "\n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(passage_entities, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "    def generate_refute_global_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "        \n",
    "        # Step 2: get entity replacement\n",
    "        entity_replacement_dict = {} # get replacement beforehand to save time\n",
    "\n",
    "        valid_entities = set()\n",
    "        for ent in passage_entities:\n",
    "            ent_text, _ = ent.split(':::')\n",
    "            replacement = self.replacement_generator.get_options(ent_text, entity=ent, passage_entities=passage_entities)\n",
    "            if replacement is not None:\n",
    "                entity_replacement_dict[ent_text] = replacement\n",
    "                valid_entities.add(ent)\n",
    "        # print(f\"entity_replacement_dict={entity_replacement_dict}\")\n",
    "\n",
    "        # Step 3: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 4: Answer Replacement\n",
    "        to_predict = []\n",
    "        replace_type = []\n",
    "        replace_keys = []\n",
    "    \n",
    "        for qa in QA_pairs:\n",
    "            ans_ent_text = qa['answer']\n",
    "            ans_ent_type = qa['answer_type']\n",
    "            if ans_ent_text == \"\" or ans_ent_type == \"\":\n",
    "                continue\n",
    "            replacement = entity_replacement_dict.get(ans_ent_text)\n",
    "            if replacement is not None:\n",
    "                # print(f'\"{ans_ent_text}:::{ans_ent_type}\" -> \"{replacement}\"')\n",
    "                # predict_input = qa['question'] + ' [SEP] ' + replacement[0] # original model\n",
    "                predict_input = qa['question'] + '</s>' + replacement[0]\n",
    "                # print(f\">>> {predict_input}\")\n",
    "                to_predict.append(predict_input)\n",
    "                replace_keys.append(f\"{ans_ent_text}:::{ans_ent_type}\")\n",
    "                replace_type.append(ans_ent_type)\n",
    "\n",
    "        # Step 5: QA2D\n",
    "        if len(to_predict) == 0:\n",
    "            return None\n",
    "        # results = []\n",
    "        # try:\n",
    "        results = self.predict(to_predict)\n",
    "            # print(f\"results={results}\")\n",
    "        # except:\n",
    "            # return None\n",
    "        if len(results) == 0:\n",
    "            return None\n",
    "        \n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(replace_keys, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "\n",
    "    def generate(self, claims_json, claim_type: str, save_every=0, cont=False):\n",
    "        claim_type = claim_type.lower()\n",
    "        assert claim_type in [\"supported\", \"refuted\"]\n",
    "        start = 0\n",
    "        if Path(claims_json).is_file():\n",
    "            if cont:\n",
    "                generated_claims = read_json(claims_json)\n",
    "                print(f\"file exists: {claims_json}, completed: {len(generated_claims)-1}/{len(self.corpus_recs)}\")\n",
    "                start = len(generated_claims)\n",
    "            else:\n",
    "                # print(\"--------------FIX!!!!!!!!!!!-------------------------\")\n",
    "                # generated_claims = read_json(claims_json)\n",
    "                raise FileExistsError(f\"File already exists: {claims_json} !!!\")\n",
    "        else:\n",
    "            generated_claims = dict() # ordered since P3.7\n",
    "        cnt = 1\n",
    "        for sample in tqdm(self.corpus_recs[start:], initial=start, total=len(self.corpus_recs)):\n",
    "            id_ = str(sample['id'])\n",
    "            if claim_type == \"supported\":\n",
    "                claims = self.generate_supported_claims(sample)\n",
    "            elif claim_type == \"refuted\":\n",
    "                claims = self.generate_refute_global_claims(sample)\n",
    "            if claims is None:\n",
    "                claims = {}\n",
    "            generated_claims[id_] = claims\n",
    "            cnt += 1\n",
    "            if save_every > 0 and cnt % save_every == 0:\n",
    "                write_json(claims_json, generated_claims)\n",
    "\n",
    "        write_json(claims_json, generated_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/550 [00:00<?, ?it/s]/home/drchajan/devel/python/FC/fc_env_plight_env/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "100%|██████████| 550/550 [09:40<00:00,  1.05s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 584/584 [10:19<00:00,  1.06s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5031/5031 [1:23:50<00:00,  1.00it/s]  \n"
     ]
    }
   ],
   "source": [
    "confs = [\n",
    "    (corpus_recs_dev, \"dev\"),\n",
    "    (corpus_recs_tst, \"test\"),\n",
    "    (corpus_recs_trn, \"train\"),\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for corpus_recs, name in confs:\n",
    "    claim_generator = ClaimGenerator(replacement_generator, \n",
    "                                 corpus_recs, \n",
    "                                 ner_json=Path(QACG_ROOT, \"ner\", f\"{name}_ners-PAV-ner-CNEC.json\"), \n",
    "                                 qas_json=Path(QACG_ROOT, \"qa\", f\"{name}_qas-PAV-ner-CNEC_mt5-large-cp59000.json\"), \n",
    "                                 QA2D_model_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qa2d/facebook/mbart-large-cc25_cs_CZ/checkpoint-26000\", \n",
    "                                 device=device)\n",
    "\n",
    "    claim_generator.generate(Path(QACG_ROOT, \"claim\", f\"{name}_sup_claims-PAV-ner-CNEC_mt5-large-cp59000.json\"), \"supported\", save_every=100)\n",
    "    # claim_generator.generate(Path(QACG_ROOT, \"claim\", f\"{name}_ref_claims-PAV-ner-CNEC_mt5-large-cp59000.json\"), \"refuted\", save_every=100)\n",
    "    # claim_generator.generate(Path(QACG_ROOT, f\"{name}_ref_claims.json\"), \"refuted\", save_every=100, cont=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('American colonists', 'NOUN')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_generator.get_options(\"King George\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env_plight_env",
   "language": "python",
   "name": "fc_env_plight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
