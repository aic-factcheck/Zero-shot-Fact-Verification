{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter, OrderedDict\n",
    "import ujson\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import sys\n",
    "import textwrap\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Set, Union\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from aic_nlp_utils.batch import batch_apply\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.fever import fever_detokenize, import_fever_corpus_from_sqlite\n",
    "from aic_nlp_utils.wiki import filter_and_fix_wiki_extract_for_lang\n",
    "\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import stanza\n",
    "# stanza.download(\"en\")\n",
    "\n",
    "sys.path.append('Claim_Generation')\n",
    "from T5_QG import pipeline\n",
    "from distractor_generation import Distractor_Generation\n",
    "\n",
    "sys.path.append('Models')\n",
    "from arguments import ModelArguments, DataTrainingArguments\n",
    "from load import load_tokenizer_and_model, find_last_checkpoint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is full version aimed at generating SUPPORTED and REFUTED claims needed for evidence retrieval, keeping NEIs for later.\n",
    "- It is a simplified version of the `claim_generation_paragraphs_wiki.ipynb`.\n",
    "- Aimed to generate data for post LREV EnFEVER models (e.g., ColBERT v2) for CsFEVER corpus.\n",
    "- Fixed input and output formats for those we use in AIC.\n",
    "\n",
    "**Notes**\n",
    "- Currently ignoring multi-hops - single evidence documents are used only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEVER_ROOT = Path(\"/mnt/data/factcheck/fever/data_full_nli-filtered-cs\")\n",
    "FEVER_DATA = Path(FEVER_ROOT, \"fever-data/F1_titles_anserininew_threshold\")\n",
    "FEVER_CORPUS_SQLITE = Path(FEVER_ROOT, \"fever/cs_wiki_revid_db_sqlite.db\")\n",
    "# QACG_TYPE = \"qacg\" # default based on FEVER evidence pages\n",
    "QACG_TYPE = \"qacg-r\" # random Wikipedia pages, about twice as many pages than \"qacg\"\n",
    "QACG_ROOT = Path(FEVER_ROOT, QACG_TYPE)\n",
    "QACG_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/wiki-pages/**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1476/1476 [00:12<00:00, 122.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# all: 825078\n",
      "# without duplicate texts: 501199\n",
      "# without short texts: 501158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501158/501158 [00:00<00:00, 636860.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# without text removed based on RE: 501142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501142/501142 [00:06<00:00, 80830.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fixed HTML: 3992, remaining: 0, errors: 0\n"
     ]
    }
   ],
   "source": [
    "EXTRACTED_ROOT = \"/mnt/data/factcheck/fever/data_full_nli-filtered-cs/\"\n",
    "corpus = filter_and_fix_wiki_extract_for_lang(\n",
    "    Path(EXTRACTED_ROOT, \"wiki-pages\"),\n",
    "    Path(EXTRACTED_ROOT, \"fever\", \"wiki_extract_filtered_and_fixed_drchajan.jsonl\"), \"cs\", textcol=\"contents\")\n",
    "corpus_id2idx = {r[\"id\"]: i for i, r in enumerate(corpus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'České středohoří',\n",
       " 'revid': '503435',\n",
       " 'url': 'https://cs.wikipedia.org/wiki?curid=4612',\n",
       " 'title': 'České středohoří',\n",
       " 'original_id': 4612,\n",
       " 'text': 'České středohoří () je geomorfologický celek o rozloze 1265 km². Z hlediska horopisného patří do Podkrušnohorské oblasti, která je součástí Krušnohorské subprovincie. Na 84 % území Českého středohoří zaujímá Chráněná krajinná oblast České středohoří (CHKO České středohoří) o výměře 1063,17 km². Nejvyšším vrcholem je Milešovka (837 m). Nejnižším bodem je hladina Labe v Děčíně (121,9 m). Maximální výškový rozdíl tedy činí 715,1 m. Geomorfologické členění. Hluboké údolí Labe rozděluje České středohoří na dva geomorfologické podcelky: Verneřické středohoří (IIIB-5A) na pravém břehu Labe a Milešovské středohoří (IIIB-5B) na levém břehu Labe. Tyto podcelky se dále člení do celkem osmi okrsků: Geologie. Rozlohou 1266 km², délkou přes 70 km a šířkou až 25 km patří České středohoří k menším orografickým celkům. Přesto je však nejmohutnějším projevem sopečné činnosti v Česku. České středohoří totiž vzniklo sopečnou činností. Podél řeky Ohře existoval Ohárecký rift, kterým se žhavé magma dostalo na povrch. V oblasti převažují čedičové horniny (73,6%), zbytek tvoří trachytické a v malé míře andezitické horniny. Územím prochází Litoměřický hlubinný zlom, který z geologického hlediska tvoří hranici mezi krušnohorskou a středočeskou oblastí. Pod povrchem se hromadilo magma v žilách a tvořily se tzv. lakolity, což byly podpovrchové balvany z utuhlého magmatu. V mladších třetihorách, v miocénu (asi před 23 miliony let) se začaly vyzdvihovat z pískovcového podloží sopečné kužely. V pliocénu (před 4,8 miliony let) vulkanity místy prorážely Českou křídovou pánev (Trosky, Kunětická hora). Vodní toky obnažily ztuhlé podpovrchové magma a prohlubovaly údolí, což dalo Českému středohoří majestátní krajinný ráz. Jedním takovým údolím je Porta Bohemica, kterou vymodelovala řeka Labe. Z výlevných hornin tu převažují čediče a znělce, z usazenin pískovce a opuky. Vrcholy. K významným vrchům kromě nejvyšší hory Milešovky (837 m) patří Hradišťany (753 m), Kletečná (706 m) a Lovoš (570 m). Ve východní části, oddělené Labem, jsou kopce poněkud nižší – nejvyšší z nich je Sedlo (727 m). Vodní toky. Osu středohoří tvoří úrodné údolí Labe. Dalším větším tokem je Ploučnice. Centrální části obou podcelků jsou odvodňovány drobnými toky směřujícími k Labi, Ploučnici a Bílině. Na labských přítocích v okolí Ústí nad Labem jsou zpětnou erozí vytvořeny vysoké vodopády (Moravanský, Vaňovský, ve Vlčí rokli, u Budova, v údolí Peklo u Týniště). Nejvyšší z nich má 12 m. Lesy. Lesnatost je malá a dosahuje 28,4 %. Z druhů je nejvíce zastoupen smrk (32,8 %) a dále se vyskytuje dub, buk, habr, bříza, jasan, lípa a javor. Drobná fauna a flóra. České středohoří má, díky své přirozené uzavřené poloze ze severu a otevřené z jihu, příznivé podmínky pro výskyt náročnějších druhů rostlin a živočichů, běžně žijících v teplých krajích. Roste zde například lilie zlatohlavá. Lidová architektura. Pro České středohoří je specifická venkovská sídelní struktura, daná hustou sítí malých obcí, osad a drobných sídel. Početné jsou památky lidové architektury, prolíná se tu několik regionálních typů lidových staveb (objekty zděné, roubené, hrázděné). O bohaté historii svědčí i několik předslovanských a slovanských hradišť, řada středověkých hradů na vrcholech kopců, tvrzí, zámků a dalších šlechtických sídel.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[2003]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following extracts corpus pages used as evidence in annotated CsFEVER data. We can use any page, but this will give us better comparison of what QACG generates when compared to FEVER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this differs from EnFEVER!\n",
    "# Tomas Mlynar uses different format...\n",
    "def extract_fever_evidence_pages(split_jsonls: List, corpus_id2idx: Dict, corpus):\n",
    "    fever_pages = set()\n",
    "    corpus_records = []\n",
    "    not_found = 0\n",
    "    for jsonl in split_jsonls:\n",
    "        print(jsonl)\n",
    "        split = read_jsonl(jsonl)\n",
    "        for rec in split:\n",
    "            if rec[\"verifiable\"] == \"VERIFIABLE\":\n",
    "                for ev in rec[\"evidence_cs\"].keys():\n",
    "                    ev = nfc(ev)\n",
    "                    if ev in corpus_id2idx:\n",
    "                        if ev not in fever_pages:\n",
    "                            corpus_records.append(corpus[corpus_id2idx[ev]])\n",
    "                        fever_pages.add(ev)\n",
    "                    else:\n",
    "                        not_found += 1\n",
    "    print(f\"missing pages: {not_found}/{not_found+len(fever_pages)}\")\n",
    "    return fever_pages, corpus_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_evidence_pages(corpus_id2idx: Dict, corpus, sizes: List[int], exclude_ids: set, seed=1234):\n",
    "    N = np.sum(sizes)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    all_ids = set(corpus_id2idx.keys())\n",
    "    all_ids = sorted(list(all_ids.difference(exclude_ids))) # sort needed for determinism\n",
    "    selected_ids = rng.choice(all_ids, N, replace=False)\n",
    "    split_ids = []\n",
    "    offset = 0\n",
    "    corpus_records = []\n",
    "    for n in sizes:\n",
    "        ids = list(selected_ids[offset:offset+n])\n",
    "        split_ids.append(ids)\n",
    "        corpus_records.append([corpus[corpus_id2idx[id_]] for id_ in ids])\n",
    "        offset += n\n",
    "    return split_ids, corpus_records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/fever-data/F1_titles_anserininew_threshold/train_fb_cs_nli_split_F1_titles_anserininew.jsonl\n",
      "missing pages: 53/5084\n",
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/fever-data/F1_titles_anserininew_threshold/paper_dev_fb_cs_nli_split_F1_titles_anserininew.jsonl\n",
      "missing pages: 3/553\n",
      "/mnt/data/factcheck/fever/data_full_nli-filtered-cs/fever-data/F1_titles_anserininew_threshold/paper_test_fb_cs_nli_split_F1_titles_anserininew.jsonl\n",
      "missing pages: 0/584\n"
     ]
    }
   ],
   "source": [
    "pages_trn, corpus_recs_trn = extract_fever_evidence_pages([Path(FEVER_DATA, \"train_fb_cs_nli_split_F1_titles_anserininew.jsonl\")], corpus_id2idx, corpus)\n",
    "pages_dev, corpus_recs_dev = extract_fever_evidence_pages([Path(FEVER_DATA, \"paper_dev_fb_cs_nli_split_F1_titles_anserininew.jsonl\")], corpus_id2idx, corpus)\n",
    "pages_tst, corpus_recs_tst = extract_fever_evidence_pages([Path(FEVER_DATA, \"paper_test_fb_cs_nli_split_F1_titles_anserininew.jsonl\")], corpus_id2idx, corpus)\n",
    "fever_pages_all = set.union(pages_trn, pages_dev, pages_tst)\n",
    "\n",
    "if QACG_TYPE == 'qacg-r':\n",
    "    (pages_trn, pages_dev, pages_tst), (corpus_recs_trn, corpus_recs_dev, corpus_recs_tst)  = extract_random_evidence_pages(corpus_id2idx, corpus, sizes=[10000, 1000, 1000], exclude_ids=fever_pages_all, seed=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_trn), len(pages_dev), len(pages_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Oleg Kononěnko',\n",
       " 'revid': '9631',\n",
       " 'url': 'https://cs.wikipedia.org/wiki?curid=421488',\n",
       " 'title': 'Oleg Kononěnko',\n",
       " 'original_id': 421488,\n",
       " 'text': 'Oleg Dmitrijevič Kononěnko (, * 21. června 1964, Čardžou, Turkmenská SSR) je od března 1996 ruský kosmonaut. Roku 2008 absolvoval půlroční kosmický let na Mezinárodní vesmírnou stanici (ISS) jako člen expedice 17, v letech 2011–2012 pracoval na ISS podruhé v Expedici 30 a 31. Pořetí na ISS pracoval jako člen Expedice 44/45 v červenci – prosinci 2015 a poté ještě jednou od prosince 2018 do června 2019 jako člen Expedice 57/58/59. Během čtyř letů pětkrát vystoupil do otevřeného vesmíru, výstupy trvaly celkem 32 hodin a 13 minut. Mládí. Oleg Kononěnko se narodil v rodině řidiče Dmitrije Kononěnka žijící ve městě Čardžou v Turkmenistánu. Po ukončení střední školy jeden rok pracoval v místních leteckých dílnách. Vystudoval Charkovský letecký institut N. J. Žukovského se specializací na letecké motory. Poté se v CKSB Progress v Samaře zabýval zpracováním dokumentace elektroinstalací kosmických družic. Kosmonaut. V polovině devadesátých let projevil zájem o profesi kosmonauta, prošel nezbytnými lékařskými prohlídkami a 9. února 1996 byl státní meziresortní komisí doporučen na post kosmonauta CKSB. Dne 29. března 1996 zástupci Roskosmosu, RKK Eněrgija a Střediska přípravy kosmonautů (CPK) rozhodli o vybrání Kononěnka mezi kandidáty na kosmonauty za CKSB a 1. června téhož roku bylo změněno jeho zařazení v CKSB-Progress z „vedoucí inženýr-konstruktér“ na „zkušební kosmonaut“. V CPK absolvoval dvouletou všeobecnou kosmickou přípravu a 20. března mu byla přiznána kvalifikace „zkušební kosmonaut“. Od října 1998 se připravoval na let na Mir. K 5. lednu 1999 byl příkazem ředitele Roskosmosu převeden z CKSB Progress do RKK Eněrgija. Samarský podnik tak ztratil svého prvního a posledního kosmonauta. V prosinci 2001 byl jmenován palubním inženýrem záložní posádky 3. návštěvní expedice na ISS, jeho velitelem byl Gennadij Padalka. Let Sojuzu TM-34 proběhl v dubnu 2002. V březnu 2002 byl zařazen do hlavní posádky Expedice 9 s Padalkou a Michaelem Finckem, start byl plánován na podzim 2003. Po havárii Columbie prošly plány letů na ISS zásadní revizí, v jejímž průběhu Kononěnko z posádky vypadl. V únoru 2007 byl vybrán do Expedice 17 ve funkci palubního inženýra. Do vesmíru odstartoval 8. dubna 2008 v Sojuzu TMA-12 se Sergejem Volkovem a Korejkou I So-jon. I So-jon se záhy vrátila v Sojuzu TMA-11 se členy předcházející Expedice 16 a Volkov s Kononěnkem zůstali společně s Garrettem Reismanem na stanici. V červnu 2008 Reismanna nahradil Gregory Chamitoff. Během letu Kononěnko dvakrát vystoupil do vesmíru, celkem strávil v otevřeném kosmu 12 hodin a 12 minut. Po šesti měsících byli Volkov a Kononěnko vystřídáni dvojicí Michael Fincke, Jurij Lončakov a 24. října 2008 přistáli v Kazachstánu. Oleg Kononěnko byl nominován jako velitel záložní posádky letu Sojuz TMA-02M, který k Mezinárodní vesmírné stanici odstartoval 7. června 2011. Zároveň se stal velitelem letu Sojuz TMA-03M s plánovaným startem 25. listopadu 2011. Členy posádky Sojuzu TMA-03M se stali ještě astronaut NASA Donald Pettit a nizozemský astronaut André Kuipers. V souvislosti se sjednocením ruských oddílů kosmonautů odešel ze společnosti RKK Eněrgija a od 22. ledna 2011 je kosmonautem Střediska přípravy kosmonautů. K druhému kosmickému letu odstartoval po několika odkladech 21. prosince 2011, s Pettitem a Kuipersem se o dva dny později připojil k posádce ISS, Expedici 30. Dne 16. února 2012 s Antonem Škaplerovem vystoupil na povrch stanice, výstup trval 6 hodin a 25 minut. Po odletu trojice déle sloužících kolegů koncem dubna byla expedice přečíslována na třicátou první, přičemž Kononěnko se stal velitelem stanice. V polovině května se k nim připojili Gennadij Padalka, Sergej Revin a Joseph Acabá, kteří přiletěli v Sojuzu TMA-04M. Z oběžné dráhy se na Zem trojice Kononěnko, Pettit, Kuipers vrátila 1. července, přistáli ve 4:47:43 UTC v kazašské stepi 148 km jihovýchodně od Džezkazganu. Let trval 192 dní, 18 hodin a 58 minut. V prosinci 2012 byl jmenován do Expedice 44/45 na ISS, plánovanou na květen – listopad 2015. Do vesmíru vzlétl v Sojuzu TMA-17M společně s Kimijou Juim a Kjellem Lindgrenem. Start Sojuzu 22. července 2015 ve 21:03 UTC se vydařil, trojice kosmonautů zamířila k ISS. Na stanici pracoval ve funkci palubního inženýra pět měsíců, 11. prosince se ve stejné sestavě posádka Sojuzu TMA-17M vrátila na Zem. Již v říjnu 2013 byl jmenován zástupce velitele oddílu kosmonautů pro vědu a výzkum, v listopadu 2016 se stal velitelem oddílu kosmonautů, při zachování pozice aktivního kosmonauta. Od konce roku 2017 se připravoval na další vesmírnou misi, v roli záložního velitele lodi Sojuz MS-09 a velitele Sojuzu MS-11. V Sojuzu MS-11 odstartoval z Bajkonuru 3. prosince 2018, společně s Davidem Saint-Jacquesem a Anne McClainovou. Téhož dne se připojili ke stanici a zůstali zde jako členové Expedice 57/58/59 do 25. června 2019, kdy přistáli v kazašské stepi. Kononěnko během mise dvakrát vystoupil do otevřeného vesmíru. Oleg Kononěnko je ženatý, má dvě děti.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_recs_trn[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER\n",
    "Not one of the CZECH NER models extracts ordinals or cardinals, i.e., numbers beyond dates. This might be a problem. We need a better one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, BertTokenizerFast\n",
    "from transformers import pipeline\n",
    "from ufal.nametag import Ner, Forms, TokenRanges, NamedEntities\n",
    "\n",
    "\n",
    "def load_czert_ner_pipeline(model_name):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    tokenizer = BertTokenizerFast(Path(model_name, \"vocab.txt\"), strip_accents=False, do_lower_case=False, truncate=True, model_max_length=512)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\", device=device)\n",
    "    def ner_pipeline_pairs(text):\n",
    "        ner_dicts = ner_pipeline(text)\n",
    "        # ner_pairs = [(e[\"word\"], e[\"entity_group\"]) for e in ner_dicts]\n",
    "        ner_pairs = [(text[e[\"start\"]:e[\"end\"]], e[\"entity_group\"]) for e in ner_dicts]\n",
    "        return ner_pairs\n",
    "    return ner_pipeline_pairs\n",
    "\n",
    "\n",
    "class UFALNERExtractor:\n",
    "    def __init__(self, model):\n",
    "        self.ner = Ner.load(model)\n",
    "        self.forms = Forms()\n",
    "        self.tokens = TokenRanges()\n",
    "        self.entities = NamedEntities()\n",
    "        self.tokenizer = self.ner.newTokenizer()\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        self.tokenizer.setText(text)\n",
    "        ners = []\n",
    "        nertypes = []\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            self.ner.recognize(self.forms, self.entities)\n",
    "            \n",
    "            entities = sorted(self.entities, key=lambda entity: (entity.start, -entity.length))\n",
    "            \n",
    "            prev_end = -1\n",
    "            for entity in entities:\n",
    "                if (entity.start + entity.length) <= prev_end: # take only the highest level entities\n",
    "                    continue\n",
    "                ners.append(\" \".join(self.forms[entity.start:entity.start+entity.length]))\n",
    "                nertypes.append(entity.type)\n",
    "                prev_end = entity.start + entity.length\n",
    "        ner_pairs = [(ner, nertype) for ner, nertype in zip(ners, nertypes)]\n",
    "        return ner_pairs\n",
    "\n",
    "\n",
    "def extract_ners(corpus_recs, ner_json, model_name):\n",
    "    # for each text gives a triplet (ner, ner_type, ner-ner_type count in text)\n",
    "    # the triplets are sorted by decreasing count\n",
    "\n",
    "    if model_name == \"ufal.nametag\":\n",
    "        ner_pipeline = UFALNERExtractor(\"/mnt/data/factcheck/ufal/ner/czech-cnec2.0-140304-no_numbers.ner\")\n",
    "    else:\n",
    "        ner_pipeline = load_czert_ner_pipeline(model_name)\n",
    "    entity_dict = OrderedDict()\n",
    "    for l in tqdm(corpus_recs):\n",
    "        text = l[\"text\"]\n",
    "        ner_pairs = ner_pipeline(text)\n",
    "        ner_cnts = Counter(ner_pairs) # their \n",
    "        ners_unique_with_counts =  [(p[0], p[1], ner_cnts[(p[0], p[1])]) for p in set(ner_pairs)]\n",
    "        ners_unique_with_counts = sorted(ners_unique_with_counts, key=lambda n: -n[2])\n",
    "        entity_dict[l[\"id\"]] = ners_unique_with_counts\n",
    "    write_json(ner_json, entity_dict, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 234.32it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 237.25it/s]\n",
      "100%|██████████| 10000/10000 [00:46<00:00, 213.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model_short=\"ufal.nametag\"\n",
    "model_name=\"ufal.nametag\"\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"ner\", f\"dev_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"ner\", f\"test_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"ner\", f\"train_ners-{model_short}.json\"), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:00<01:14, 13.29it/s]/home/drchajan/devel/python/FC/fc_env_plight_env/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [00:16<00:00, 59.76it/s]\n",
      "100%|██████████| 1000/1000 [00:16<00:00, 60.79it/s]\n",
      "100%|██████████| 10000/10000 [02:42<00:00, 61.40it/s]\n"
     ]
    }
   ],
   "source": [
    "model_short=\"CZERT-B-ner-CNEC\"\n",
    "model_name=f\"/mnt/data/factcheck/models/czert/{model_short}\"\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"ner\", f\"dev_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"ner\", f\"test_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"ner\", f\"train_ners-{model_short}.json\"), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:16<00:00, 60.50it/s]\n",
      "100%|██████████| 1000/1000 [00:16<00:00, 61.93it/s]\n",
      "100%|██████████| 10000/10000 [02:45<00:00, 60.48it/s]\n"
     ]
    }
   ],
   "source": [
    "model_short=\"PAV-ner-CNEC\"\n",
    "model_name=f\"/mnt/data/factcheck/models/czert/{model_short}\"\n",
    "extract_ners(corpus_recs_dev, Path(QACG_ROOT, \"ner\", f\"dev_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_tst, Path(QACG_ROOT, \"ner\", f\"test_ners-{model_short}.json\"), model_name)\n",
    "extract_ners(corpus_recs_trn, Path(QACG_ROOT, \"ner\", f\"train_ners-{model_short}.json\"), model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not merging outputs of multiple NER methods anymore -- keeping for future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ners(fins, fout):\n",
    "    iners = [read_json(fin) for fin in fins]\n",
    "    page2ners = defaultdict(dict)\n",
    "    for iner in iners:\n",
    "        for page, ners in iner.items():\n",
    "            for ner in ners:\n",
    "                word, type_, cnt = ner\n",
    "                if word in page2ners[page]:\n",
    "                    if page2ners[page][word][1] >= cnt:\n",
    "                        continue\n",
    "                page2ners[page][word] = [type_, cnt]\n",
    "    page2nerlsts = {page: sorted([[word] + params for word, params in ners.items()], key=lambda n: -n[2]) for page, ners in page2ners.items()}\n",
    "    write_json(fout, page2nerlsts)\n",
    "\n",
    "merge_ners(\n",
    "    [Path(QACG_ROOT, \"ner\", \"dev_ners-CZERT-B-ner-CNEC.json\"),\n",
    "     Path(QACG_ROOT, \"ner\", \"dev_ners-PAV-ner-CNEC.json\")], \n",
    "    Path(QACG_ROOT, \"ner\", \"dev_ners.json\"))\n",
    "\n",
    "merge_ners(\n",
    "    [Path(QACG_ROOT, \"ner\", \"test_ners-CZERT-B-ner-CNEC.json\"),\n",
    "     Path(QACG_ROOT, \"ner\", \"test_ners-PAV-ner-CNEC.json\")], \n",
    "    Path(QACG_ROOT, \"ner\", \"test_ners.json\"))\n",
    "\n",
    "merge_ners(\n",
    "    [Path(QACG_ROOT, \"ner\", \"train_ners-CZERT-B-ner-CNEC.json\"),\n",
    "     Path(QACG_ROOT, \"ner\", \"train_ners-PAV-ner-CNEC.json\")], \n",
    "    Path(QACG_ROOT, \"ner\", \"train_ners.json\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation (QG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchQuestionGenerator:\n",
    "    def __init__(self, tokenizer, model, highlight=False, highlight_tag=\"<hl>\", max_source_length=1024, padding=False, device=\"cuda\", debug=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(device)\n",
    "        self.highlight = highlight\n",
    "        self.highlight_tag = highlight_tag\n",
    "        self.max_source_length = max_source_length\n",
    "        self.padding = padding\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "\n",
    "    def generate(self, contexts, answers, batch_size=32):\n",
    "        def highlight_fun(answer, context):\n",
    "            offset = context.index(answer)\n",
    "            return f\"{context[:offset]}<hl>{answer}<hl>{context[offset + len(answer):]}\"\n",
    "\n",
    "        n = len(contexts)\n",
    "        assert n == len(answers), (n, len(answers))\n",
    "        offset = 0\n",
    "        failures = 0\n",
    "        predictions = []\n",
    "        while offset < n:\n",
    "            last = min(offset + batch_size, n)\n",
    "            if self.highlight:\n",
    "                inputs = []\n",
    "                for context, answer in zip(contexts[offset:last], answers[offset:last]):\n",
    "                    # if answer in context:\n",
    "                    inputs.append(highlight_fun(answer, context) )\n",
    "                    # else:\n",
    "                        # failures += 1\n",
    "            else:\n",
    "                inputs = [answer + \"</s>\" + context for context, answer in zip(contexts[offset:last], answers[offset:last])]\n",
    "            model_inputs = self.tokenizer(inputs, max_length=self.max_source_length, padding=self.padding, truncation=True, return_tensors=\"pt\")\n",
    "            model_inputs = {k: v.to(self.device) for k, v in model_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                Y = self.model.generate(**model_inputs, max_new_tokens=768)\n",
    "                batch_predictions = self.tokenizer.batch_decode(\n",
    "                    Y, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            predictions += batch_predictions\n",
    "            offset += batch_size\n",
    "\n",
    "        assert n == len(predictions)\n",
    "        if self.debug:\n",
    "            for input, pred in zip(inputs, predictions):\n",
    "                print(textwrap.fill(input))\n",
    "                print()\n",
    "                print(pred)\n",
    "                print(\"----------------------------\")\n",
    "        # print(f\"#failures: {failures}, #predictions: {len(predictions)}/{n}\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight = False\n",
    "# def batch_evaluator():\n",
    "# model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/facebook/mbart-large-cc25_cs_CZ/checkpoint-10000\")\n",
    "# model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/google/mt5-base_cs_CZ/checkpoint-40000\")\n",
    "model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/google/mt5-large_cs_CZ/checkpoint-59000\")\n",
    "\n",
    "# highlight = True\n",
    "# model_args = ModelArguments(model_name_or_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qg/google/mt5-large_cs_CZ_hl/checkpoint-48000\")\n",
    "\n",
    "tokenizer, model, data_collator = load_tokenizer_and_model(model_args, lang=\"cs_CZ\", fp16=True)\n",
    "\n",
    "batch_question_generator = BatchQuestionGenerator(tokenizer, model, highlight=highlight, padding=True, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qas(corpus_recs, ner_json, qas_json, generator):\n",
    "    # QG NLP object\n",
    "\n",
    "    # print('Loading QG module >>>>>>>>')\n",
    "    # print('QG module loaded.')\n",
    "\n",
    "    ners = read_json(ner_json)\n",
    "\n",
    "    qas = OrderedDict()\n",
    "    invalid_sample = 0\n",
    "    for l in tqdm(corpus_recs):\n",
    "        id_ = str(l['id'])\n",
    "        if id_ not in ners: # no NERs in this text\n",
    "            continue\n",
    "        entities = ners[id_]\n",
    "\n",
    "        # create a batch\n",
    "        contexts, answers = [], []\n",
    "        for ent_text, ent_type, ent_cnt in entities:\n",
    "            contexts.append(l['text'])\n",
    "            answers.append(ent_text)\n",
    "\n",
    "\n",
    "        # question generation\n",
    "        if len(contexts) > 0 and len(contexts) == len(answers):\n",
    "            questions = []\n",
    "            # try:\n",
    "            questions = generator.generate(contexts, answers)\n",
    "            # except:\n",
    "                # invalid_sample += 1\n",
    "\n",
    "            if len(questions) == 0:\n",
    "                continue\n",
    "            \n",
    "            assert len(questions) == len(contexts)\n",
    "            # save results\n",
    "            result_for_sample = {}\n",
    "            for entity, question, answer, context in zip(entities, questions, answers, contexts):\n",
    "                ent_text, ent_type, _ = entity\n",
    "                result_for_sample[f'{ent_text}:::{ent_type}'] = [question, answer]\n",
    "\n",
    "            qas[str(l['id'])] = result_for_sample\n",
    "        else:\n",
    "            invalid_sample += 1\n",
    "\n",
    "    # print(f'#invalid samples: {invalid_sample}')\n",
    "    Path(qas_json).parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_json(qas_json, qas)\n",
    "\n",
    "\n",
    "# generate_qas(corpus_recs_dev[0:1], Path(QACG_ROOT, \"ner\", \"dev_ners.json\"), Path(QACG_ROOT, \"qa\", \"dev_qas.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_dev, Path(QACG_ROOT, \"ner\", \"dev_ners-PAV-ner-CNEC.json\"), Path(QACG_ROOT, \"qa\", \"dev_qas-PAV-ner-CNEC_cp10000.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_dev, Path(QACG_ROOT, \"ner\", \"dev_ners-PAV-ner-CNEC.json\"), Path(QACG_ROOT, \"qa\",\"dev_qas-PAV-ner-CNEC_mt5-base-cp40000.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_tst, Path(QACG_ROOT, \"ner\", \"test_ners.json\"), Path(QACG_ROOT, \"qa\", \"test_qas.json\"), batch_question_generator)\n",
    "# generate_qas(corpus_recs_trn, Path(QACG_ROOT, \"ner\", \"train_ners.json\"), Path(QACG_ROOT, \"qa\", \"train_qas.json\"), batch_question_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [23:52<00:00,  1.43s/it]\n",
      "100%|██████████| 1000/1000 [22:24<00:00,  1.34s/it]\n",
      "100%|██████████| 10000/10000 [3:57:14<00:00,  1.42s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/data/factcheck/fever/data_full_nli-filtered-cs/qacg-r')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confs = [                        \n",
    "    (corpus_recs_dev, \"dev\"),\n",
    "    (corpus_recs_tst, \"test\"),\n",
    "    (corpus_recs_trn, \"train\"),\n",
    "]\n",
    "\n",
    "for corpus_recs, name in confs:\n",
    "    generate_qas(corpus_recs, Path(QACG_ROOT, \"ner\", f\"{name}_ners-PAV-ner-CNEC.json\"), Path(QACG_ROOT, \"qa\", f\"{name}_qas-PAV-ner-CNEC_mt5-large-cp59000.json\"), batch_question_generator)\n",
    "\n",
    "QACG_ROOT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim Generator (QA2D + Replacement Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for testing\n",
    "# QA2D_model_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qa2d/facebook/mbart-large-cc25_cs_CZ/checkpoint-26000\"\n",
    "# sense_to_vec_path=\"dependencies/s2v_old\"\n",
    "\n",
    "# model_args = ModelArguments(model_name_or_path=QA2D_model_path)\n",
    "# tokenizer, model, data_collator = load_tokenizer_and_model(model_args, lang=\"cs_CZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameDocumentNERReplacementGenerator:\n",
    "    def __init__(self, seed=1234):\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "    def get_options(self, answer, entity, passage_entities, **kwargs):\n",
    "        ent_name, ent_type = entity.split(\":::\")\n",
    "        selected_entity_names = set()\n",
    "        for passage_entity in passage_entities:\n",
    "            pent_name, pent_type = passage_entity.split(\":::\")\n",
    "            if pent_type == ent_type and pent_name != ent_name:\n",
    "                selected_entity_names.add(pent_name)\n",
    "        if len(selected_entity_names) == 0:\n",
    "            return None\n",
    "        selected_entity_names = list(selected_entity_names)\n",
    "        selected_entity_name = self.rng.choice(selected_entity_names)\n",
    "        selected_entity = (selected_entity_name, ent_type)\n",
    "        # print(f\"{entity} -> {selected_entity}\")\n",
    "        return selected_entity\n",
    "    \n",
    "# replacement_generator = Distractor_Generation(sense2vec_path=sense_to_vec_path, T=0.7) # original EN replacement generator\n",
    "replacement_generator = SameDocumentNERReplacementGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimGenerator:\n",
    "    def __init__(self, replacement_generator, corpus_recs, ner_json, qas_json, QA2D_model_path, device=\"cuda\"):\n",
    "        # QA2D model object\n",
    "        print('Loading QA2D module >>>>>>>>')\n",
    "        model_args = ModelArguments(model_name_or_path=QA2D_model_path)\n",
    "        self.tokenizer, self.model, data_collator = load_tokenizer_and_model(model_args, lang=\"cs_CZ\")\n",
    "        print(f'Running on device: {device}')\n",
    "        # self.model, self.tokenizer = model, tokenizer # TODO REMOVE\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.replacement_generator = replacement_generator\n",
    "\n",
    "        self.corpus_recs = corpus_recs\n",
    "        self.ners = read_json(ner_json)\n",
    "        self.qas = read_json(qas_json)\n",
    "\n",
    "    def predict(self, inputs, max_source_length=1024, batch_size=16):\n",
    "        def pred_func(input_texts: List[str]) -> List[str]:\n",
    "            with torch.no_grad():\n",
    "                X = self.tokenizer(input_texts, max_length=max_source_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                X = {k: X[k].to(self.device) for k in X.keys()}\n",
    "                Y = self.model.generate(**X, max_new_tokens=768)\n",
    "                output_texts = self.tokenizer.batch_decode(\n",
    "                    Y, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            return output_texts\n",
    "            \n",
    "        predictions = batch_apply(pred_func, inputs, batch_size=batch_size)\n",
    "        return predictions\n",
    "\n",
    "    def _load_passage_entities(self, id_):\n",
    "        passage_entities = []\n",
    "        for ent_text, ent_type, _ in self.ners[id_]:\n",
    "            passage_entities.append(f'{ent_text}:::{ent_type}') # group by entity name and type as in the QAS file\n",
    "        return passage_entities\n",
    "    \n",
    "    def _load_precomputed_qas_for_entities(self, id_, passage_entities):\n",
    "        if id_ not in self.qas:\n",
    "            print(f\"missing id: {id_}\")\n",
    "            return None\n",
    "        QA_for_sample = self.qas[id_]\n",
    "        QA_pairs = []\n",
    "        for entity in passage_entities:\n",
    "            if entity in QA_for_sample:\n",
    "                ent_text, ent_type = entity.split(':::')\n",
    "                question, answer = QA_for_sample[entity]\n",
    "                QA_pairs.append({'question': question, 'answer': answer, 'answer_type': ent_type})\n",
    "            else:\n",
    "                print(f\"missing entity: {entity} for id: {id_}\")\n",
    "                return None\n",
    "        if len(QA_pairs) == 0:\n",
    "            print(f\"zero length pairs for id: {id_}\")\n",
    "            return None\n",
    "        return QA_pairs \n",
    "        \n",
    "\n",
    "    def generate_supported_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "\n",
    "        # Step 2: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 3: QA2D\n",
    "        # to_predict = [qa['question'] + ' [SEP] ' + qa['answer'] for qa in QA_pairs] # original model\n",
    "        to_predict = [qa['answer'] + '</s>' + qa['question'] for qa in QA_pairs]\n",
    "        results = []\n",
    "        # try:\n",
    "        results = self.predict(to_predict)\n",
    "        # except:\n",
    "            # return None\n",
    "        if len(results) == 0:\n",
    "            print(f\"zero length results for id: {id_}\")\n",
    "            return None\n",
    "\n",
    "        assert len(results) == len(QA_pairs)\n",
    "\n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(passage_entities, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "    def generate_refute_global_claims(self, sample):\n",
    "        texts, id_ = sample['text'], str(sample['id'])\n",
    "\n",
    "        # Step 1: load entities in text\n",
    "        passage_entities = self._load_passage_entities(id_)\n",
    "        if len(passage_entities) == 0: # no NERs\n",
    "            return None \n",
    "        \n",
    "        # Step 2: get entity replacement\n",
    "        entity_replacement_dict = {} # get replacement beforehand to save time\n",
    "\n",
    "        valid_entities = set()\n",
    "        for ent in passage_entities:\n",
    "            ent_text, _ = ent.split(':::')\n",
    "            replacement = self.replacement_generator.get_options(ent_text, entity=ent, passage_entities=passage_entities)\n",
    "            if replacement is not None:\n",
    "                entity_replacement_dict[ent_text] = replacement\n",
    "                valid_entities.add(ent)\n",
    "        # print(f\"entity_replacement_dict={entity_replacement_dict}\")\n",
    "\n",
    "        # Step 3: load precomputed QAs for entities\n",
    "        QA_pairs = self._load_precomputed_qas_for_entities(id_, passage_entities)\n",
    "        if QA_pairs is None:\n",
    "            return None\n",
    "\n",
    "        # Step 4: Answer Replacement\n",
    "        to_predict = []\n",
    "        replace_type = []\n",
    "        replace_keys = []\n",
    "    \n",
    "        for qa in QA_pairs:\n",
    "            ans_ent_text = qa['answer']\n",
    "            ans_ent_type = qa['answer_type']\n",
    "            if ans_ent_text == \"\" or ans_ent_type == \"\":\n",
    "                continue\n",
    "            replacement = entity_replacement_dict.get(ans_ent_text)\n",
    "            if replacement is not None:\n",
    "                # print(f'\"{ans_ent_text}:::{ans_ent_type}\" -> \"{replacement}\"')\n",
    "                # predict_input = qa['question'] + ' [SEP] ' + replacement[0] # original model\n",
    "                predict_input = qa['question'] + '</s>' + replacement[0]\n",
    "                # print(f\">>> {predict_input}\")\n",
    "                to_predict.append(predict_input)\n",
    "                replace_keys.append(f\"{ans_ent_text}:::{ans_ent_type}\")\n",
    "                replace_type.append(ans_ent_type)\n",
    "\n",
    "        # Step 5: QA2D\n",
    "        if len(to_predict) == 0:\n",
    "            return None\n",
    "        # results = []\n",
    "        # try:\n",
    "        results = self.predict(to_predict)\n",
    "            # print(f\"results={results}\")\n",
    "        # except:\n",
    "            # return None\n",
    "        if len(results) == 0:\n",
    "            return None\n",
    "        \n",
    "        claims_for_sample = OrderedDict()\n",
    "        for ent, claim in zip(replace_keys, results):\n",
    "            claims_for_sample[ent] = claim\n",
    "        return claims_for_sample\n",
    "\n",
    "\n",
    "    def generate(self, claims_json, claim_type: str, save_every=0, cont=False):\n",
    "        claim_type = claim_type.lower()\n",
    "        assert claim_type in [\"supported\", \"refuted\"]\n",
    "        start = 0\n",
    "        if Path(claims_json).is_file():\n",
    "            if cont:\n",
    "                generated_claims = read_json(claims_json)\n",
    "                print(f\"file exists: {claims_json}, completed: {len(generated_claims)-1}/{len(self.corpus_recs)}\")\n",
    "                start = len(generated_claims)\n",
    "            else:\n",
    "                # print(\"--------------FIX!!!!!!!!!!!-------------------------\")\n",
    "                # generated_claims = read_json(claims_json)\n",
    "                raise FileExistsError(f\"File already exists: {claims_json} !!!\")\n",
    "        else:\n",
    "            generated_claims = dict() # ordered since P3.7\n",
    "        cnt = 1\n",
    "        for sample in tqdm(self.corpus_recs[start:], initial=start, total=len(self.corpus_recs)):\n",
    "            id_ = str(sample['id'])\n",
    "            if claim_type == \"supported\":\n",
    "                claims = self.generate_supported_claims(sample)\n",
    "            elif claim_type == \"refuted\":\n",
    "                claims = self.generate_refute_global_claims(sample)\n",
    "            if claims is None:\n",
    "                claims = {}\n",
    "            generated_claims[id_] = claims\n",
    "            cnt += 1\n",
    "            if save_every > 0 and cnt % save_every == 0:\n",
    "                write_json(claims_json, generated_claims, mkdir=True)\n",
    "\n",
    "        write_json(claims_json, generated_claims, mkdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/drchajan/devel/python/FC/fc_env_plight_env/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [11:56<00:00,  1.39it/s]\n",
      "100%|██████████| 1000/1000 [10:43<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:36<00:00,  1.57it/s]\n",
      "100%|██████████| 1000/1000 [09:33<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA2D module >>>>>>>>\n",
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:56:04<00:00,  1.44it/s] \n",
      "100%|██████████| 10000/10000 [1:49:15<00:00,  1.53it/s] \n"
     ]
    }
   ],
   "source": [
    "lang = \"cs_CZ\"\n",
    "\n",
    "confs = [\n",
    "    (corpus_recs_dev, \"dev\"),\n",
    "    (corpus_recs_tst, \"test\"),\n",
    "    (corpus_recs_trn, \"train\"),\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for corpus_recs, name in confs:\n",
    "    claim_generator = ClaimGenerator(replacement_generator, \n",
    "                                 corpus_recs, \n",
    "                                 ner_json=Path(QACG_ROOT, \"ner\", f\"{name}_ners-PAV-ner-CNEC.json\"), \n",
    "                                 qas_json=Path(QACG_ROOT, \"qa\", f\"{name}_qas-PAV-ner-CNEC_mt5-large-cp59000.json\"), \n",
    "                                 QA2D_model_path=\"/home/drchajan/devel/python/FC/Zero-shot-Fact-Verification/experiments/qa2d/facebook/mbart-large-cc25_cs_CZ/checkpoint-26000\", \n",
    "                                 lang=lang,\n",
    "                                 device=device)\n",
    "\n",
    "    claim_generator.generate(Path(QACG_ROOT, \"claim\", f\"{name}_sup_claims-PAV-ner-CNEC_mt5-large-cp59000.json\"), \"supported\", save_every=100)\n",
    "    claim_generator.generate(Path(QACG_ROOT, \"claim\", f\"{name}_ref_claims-PAV-ner-CNEC_mt5-large-cp59000.json\"), \"refuted\", save_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('American colonists', 'NOUN')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_generator.get_options(\"King George\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflarge",
   "language": "python",
   "name": "hflarge"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
